version: 4
data:
  attachedData:
    trivet:
      testSuites: []
      version: 1
  graphs:
    1C7HNzd5v7ULqf04-5Uob:
      metadata:
        description: ""
        id: 1C7HNzd5v7ULqf04-5Uob
        name: RA - Recall Memories
      nodes:
        '[9HlRtZ5rPSP7q5FiAHjP4]:graphOutput "Graph Output"':
          data:
            dataType: object[]
            id: items
          visualData: 1633.1906333726722/443.91188062401466/300/31
        '[QD7tiW9UZpo6N30A5foaD]:extractObjectPath "Extract Object Path"':
          data:
            path: $.topic
            usePathInput: false
          visualData: 683/375/250/2
        '[UqDCzlc17MI37nenyQCWR]:graphInput "Graph Input"':
          data:
            dataType: string
            defaultValue: This is a test topic
            id: topic
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Get Embedding" u0PxZtqctJpURQO38Gg9s/input
          visualData: 316.85283656184424/219.3939539319971/300/29
        '[ZljA5lFqK9KWtpwOGZFSZ]:vectorNearestNeighbors "Vector KNN"':
          data:
            collectionId: rivet-3ff65ea.svc.us-west1-gcp-free.pinecone.io/test1
            integration: pinecone
            k: 10
            useKInput: true
          outgoingConnections:
            - results->"Graph Output" 9HlRtZ5rPSP7q5FiAHjP4/value
          visualData: 1279.3360881667509/408.90705286005004/200/24
        '[u0PxZtqctJpURQO38Gg9s]:getEmbedding "Get Embedding"':
          data:
            integration: openai
            useIntegrationInput: false
          outgoingConnections:
            - embedding->"Vector KNN" ZljA5lFqK9KWtpwOGZFSZ/vector
          visualData: 1033.9087435447254/383.59221170727943/200/23
        '[uL9pTJZMx-cuabCpqrtFI]:graphInput "Graph Input"':
          data:
            dataType: number
            defaultValue: "3"
            id: n
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Vector KNN" ZljA5lFqK9KWtpwOGZFSZ/k
          visualData: 313.1910654160657/406.14428236670517/300/30
    3BWUibK-Zr_i2GjKA277_:
      metadata:
        description: ""
        id: 3BWUibK-Zr_i2GjKA277_
        name: RA - Extract Commands
      nodes:
        '[U-sUa3iW4WAjSKQNC8_1e]:graphInput "Graph Input"':
          data:
            dataType: string
            id: input
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Extract YAML" WLtSibdAgyBDWM6gSuUhR/input
          visualData: 259/369/300/1
        '[WLtSibdAgyBDWM6gSuUhR]:extractYaml "Extract YAML"':
          data:
            objectPath: $.systemCommands[*]
            rootPropertyName: systemCommands
          outgoingConnections:
            - matches->"Graph Output" g3sxYEM5E-xJYvcuuGZRB/value
          visualData: 613/378/250/3
        '[g3sxYEM5E-xJYvcuuGZRB]:graphOutput "Graph Output"':
          data:
            dataType: object[]
            id: commands
          visualData: 974.8003300330032/379.94224422442244/300/9
    49ADNSJiXKFRvgaRMhFvP:
      metadata:
        description: ""
        id: 49ADNSJiXKFRvgaRMhFvP
        name: RA - Get Response
      nodes:
        '[6P8d2awK1kumFWb15ecLK]:subGraph "Subgraph"':
          data:
            graphId: HRMMlTL5W-Wau-f7nLLEh
          outgoingConnections:
            - system_prompt->"Chat" 8WoCCKIA_CkgUFUS9Z2ZI/systemPrompt
          visualData: 1456.4238678422316/454.96652023842444/300/51
        '[8WoCCKIA_CkgUFUS9Z2ZI]:chat "Chat"':
          data:
            cache: false
            enableFunctionUse: false
            frequencyPenalty: 0
            maxTokens: 4096
            model: gpt-4-0613
            presencePenalty: 0
            stop: ""
            temperature: 0.5
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
            useUserInput: false
          outgoingConnections:
            - response->"Graph Output" kviYXdprAn_78olEtAd-6/value
            - response->"Prompt" iJA7SfvwwBOoyE88k97cT/input
            - response->"Subgraph" yqSRYu7edOEf3tZQaKswg/input
          visualData: 1965.1016252864763/547.5345614232717/200/67
        '[9brCG6eTcOIXX31erjqGu]:assemblePrompt "Assemble Prompt"':
          outgoingConnections:
            - prompt->"Graph Output" DL8bb2mbzNz2rSU2s7KUu/value
          visualData: 2872.693974555739/713.594640498702/250/null
        '[DL8bb2mbzNz2rSU2s7KUu]:graphOutput "Graph Output"':
          data:
            dataType: chat-message[]
            id: new_messages
          visualData: 3297.258302221817/669.5769681505825/300/61
        '[DjJurzETlQAbzGr1rHdrc]:graphInput "Graph Input"':
          data:
            dataType: string
            defaultValue: This is a test
            id: prompt_message
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Prompt" _ALVrW2X1sanCPkXgVBtx/input
          visualData: 544.1815668099302/926.8873241664447/300/63
        '[Kc9vHiGqFgor7QZhi1ezN]:trimChatMessages "Trim Chat Messages"':
          data:
            maxTokenCount: 4096
            model: gpt-4
            removeFromBeginning: true
          outgoingConnections:
            - trimmed->"Chat" 8WoCCKIA_CkgUFUS9Z2ZI/prompt
          visualData: 1658.6972703882811/613.0651361023686/200/55
        '[_ALVrW2X1sanCPkXgVBtx]:prompt "Prompt"':
          data:
            enableFunctionCall: false
            name: SYSTEM INFORMATION
            promptText: "{{input}}"
            type: user
            useTypeInput: false
          outgoingConnections:
            - output->"Assemble Prompt" uy7Kir4TGhQ-7vrOelzOG/message2
          visualData: 949.3527093687692/918.8482141950393/250/65
        '[iJA7SfvwwBOoyE88k97cT]:prompt "Prompt"':
          data:
            enableFunctionCall: false
            promptText: "{{input}}"
            type: assistant
            useTypeInput: false
          outgoingConnections:
            - output->"Assemble Prompt" 9brCG6eTcOIXX31erjqGu/message2
          visualData: 2294.628246648416/401.9702599338506/250/16
        '[kviYXdprAn_78olEtAd-6]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: chat_output
          visualData: 3321.346081191017/948.6316276694499/300/66
        '[raxkQiq1D9gedJiuQYUDd]:graphInput "Graph Input"':
          data:
            dataType: chat-message[]
            id: current_messages
          outgoingConnections:
            - data->"Assemble Prompt" uy7Kir4TGhQ-7vrOelzOG/message1
          visualData: 833.2481831324469/704.4215206372613/300/57
        '[s_dZ9oRH1p1IJCuyKuyg3]:subGraph "Subgraph"':
          data:
            graphId: u6yVHvgJi01zZYY_5f4y3
          outgoingConnections:
            - commands_output->"Assemble Prompt" 9brCG6eTcOIXX31erjqGu/message3
          visualData: 2560.6006996846772/602.3956477259945/196.3548034934497/13
        '[uy7Kir4TGhQ-7vrOelzOG]:assemblePrompt "Assemble Prompt"':
          outgoingConnections:
            - prompt->"Assemble Prompt" 9brCG6eTcOIXX31erjqGu/message1
            - prompt->"Trim Chat Messages" Kc9vHiGqFgor7QZhi1ezN/input
          visualData: 1313.7972976989524/681.9189138753514/250/58
        '[yqSRYu7edOEf3tZQaKswg]:subGraph "Subgraph"':
          data:
            graphId: 3BWUibK-Zr_i2GjKA277_
          outgoingConnections:
            - commands->"Subgraph" s_dZ9oRH1p1IJCuyKuyg3/commands
          visualData: 2264.247517768967/599.2763797815367/202.5425764192139/11
    5S6OjvRiN5DSNiYVUxf2d:
      metadata:
        description: ""
        id: 5S6OjvRiN5DSNiYVUxf2d
        name: Graph Maker
      nodes:
        '[-ELvIeQcb0AtnN6y0Y4w9]:prompt "Prompt"':
          data:
            enableFunctionCall: false
            promptText: >-
              You are an AI for creating graphs in Rivet based on a user's
              specifications. You know everything you can do with Rivet, and you
              create an entire graph in plain english based on the user's
              request.


              Rivet Specification:


              {{rivet}}



              # Goal


              The user will ask you to create a graph. You must output a series of steps you will take to great a graph like the user has asked for.


              For example:


              - Add a X node and name it "Foo"

              - Add a Y node and name it "Bar"

              - Connect the "x" output of Foo to the Y output of Bar

              - Etc


              Your steps should be extremely thorough, complete, and in plain english.
            type: system
            useTypeInput: false
          outgoingConnections:
            - output->"Chat" Sao72Qnzs9LA6eJXT5FX8/systemPrompt
          visualData: 1262/546/250/3
        '[Sao72Qnzs9LA6eJXT5FX8]:chat "Chat"':
          data:
            cache: false
            enableFunctionUse: false
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0
            stop: ""
            temperature: 0.5
            top_p: 1
            useAsGraphPartialOutput: true
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
            useUserInput: false
          visualData: 1639/620/200/5
        '[hn39Ds4GBHK0C8LkBmA5X]:prompt "Prompt"':
          data:
            enableFunctionCall: false
            promptText: Can you give me a graph that would be suitable as a tutorial?
            type: user
            useTypeInput: false
          visualData: 1131/925/250/null
        '[qp4n1iV7rXEYXlfWsXUkl]:text "Text"':
          data:
            text: >-
              # Rivet


              Rivet is a graph-based tool for creating AI large languagte models chains and AI agents. Rivet works by linking together nodes using their input and output ports. Data flows through the wires to the next node. The graphs start at nodes without inputs, and proceed to nodes without outputs. Graphs are turing-complete due to the variety of nodes available.


              ## Nodes


              Available nodes are:


              #### Text Node


              Outputs a string of text. It can also interpolate values using interpolation tags. Inputs are dynamic based on the tags (words inside {{}}).


              #### Prompt Node


              Outputs a Chat Message, which is a string of text with an attached "Assistant", "User", or "System" tag, and optionally a message.


              #### Chunk Node


              Splits the input text into an array of chunks based on an approximate GPT token count per chunk.


              #### Extract With Regex Node


              Extracts data from the input text using the configured regular expression. The regular expression can contain capture groups to extract specific parts of the text.


              Each capture group corresponds to an output port of the node.


              #### Chat Node


              Performs a chat with OpenAI. Takes two inputs - a system prompt (optional) and a prompt (which is either a list of strings, chat-messages, or a single string/chat-message). Outputs the response from the AI.


              #### Assemble Prompt Node


              Takes in a list of strings or chat-messages or arrays of each and outputs a flat list of chat messages for use with a Chat Node.


              #### Array Node


              Creates an array from the input values. Number of inputs is unlimited. Outputs the array, and its length, and the indexes.


              #### Pop Node


              Pops the last value off the input array and outputs the new array and the popped value.


              #### Number Node


              Outputs a configurable number.


              #### Extract JSON


              Given a string, extracts a JSON object out of the string, parses it, and outputs the object.


              #### Extract YAML


              Given a string, extracts a YAML object out of the string (must start with `yamlDocument:` by default) and outputs the object.


              #### Extract Object Path


              Uses jsonpath to extract something out of an object. Outputs the extracted value.


              #### Bool Node


              Outputs a boolean value.


              #### Match Node


              Configured with a list of strings. Each string is a regular expression. For the matching regex, the output port with the same index is given the input string. All other output ports are not ran.


              #### If Node


              Two inputs - if and value. If the `if` value is truthy, the `value` is passed through. Otherwise, the output is not ran.


              #### If/Else Node


              Three inputs, `if`, `true`, and `false`. If the `if` value is truthy, the `true` value is passed through. Otherwise, the `false` value is passed through.


              #### Loop Controller Node


              Marks a loop. The `continue` input specifies whether the loop should continue after the first iteration. Contains unlimited pairs of values. For example, `input1` is the value that should be output for `output2`, except for the first iteration, which takes its value from the `input1Default` input. Nodes inside the loop should wrap around to connect to the `inputX` inputs or the `continue` input. The `break` output is executed when `continue` is false, and has the list of values.


              #### Coalesce Node


              Any number of inputs. Takes the first non-null input and outputs it.


              #### Graph Output Node


              Each instance of this node represents an individual output of the graph. The value passed into this node becomes part of the overall output of the graph. The `id` data of this specifies the name of the output.


              #### Graph Input Node 


              Defines an input for the graph which can be passed in when the graph is called, or defines one of the input ports when the graph is a subgraph. The `id` data of this specifies the name of the input.


              #### User Input Node


              Prompts the user for input during the execution of the graph. The user's response becomes the output of this node. Can be configured to either prompt with and input port, or with a static configured text.


              #### Subgraph Node


              Calls a subgraph as a node. The inputs of the node come from the graph's Graph Input Nodes, and the outputs of the node come from the Graph Output Nodes.


              #### External Call Node


              Calls a function in the external library that is executing Rivet. 


              #### Code Node


              Executes arbitrary JavaScript code. Takes in one input and has one output.
          outgoingConnections:
            - output->"Prompt" -ELvIeQcb0AtnN6y0Y4w9/rivet
          visualData: 845/412/300/4
    7zpW_cdTXlsQbF5nNcgKP:
      metadata:
        description: ""
        id: 7zpW_cdTXlsQbF5nNcgKP
        name: "RA - Command: ASK_QUESTION_ABOUT_FILE"
      nodes:
        '[2338y8qNrzrbfDmzXJvhr]:chunk "Chunk"':
          data:
            model: gpt-4
            numTokensPerChunk: 4096
            overlap: 0
            useModelInput: false
          outgoingConnections:
            - chunks->"Text" nELWKzIKfsG5PysjF-cSg/contents
            - count->"Text" BiOEV7VKd8ZRQz8vwM9Ng/count
            - count->"Text" gpA-l85r9DUJnZux3JRjt/count
            - count->"Text" nELWKzIKfsG5PysjF-cSg/count
            - indexes->"Text" gpA-l85r9DUJnZux3JRjt/index
            - indexes->"Text" nELWKzIKfsG5PysjF-cSg/index
          visualData: 1200.8357446452283/346.2666438124874/200/23
        '[2_eg1SZ7lUW33gIXYfrVK]:graphInput "Graph Input"':
          data:
            dataType: object
            id: arguments
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Extract Object Path" ei3qRQVhPSrVddaO5bU-U/object
            - data->"Extract Object Path" m9kiRPWzz-lraxQy_98B8/object
          visualData: 239/357/300/1
        '[BiOEV7VKd8ZRQz8vwM9Ng]:text "Text"':
          data:
            text: >-
              A question was asked about the file {{file_name}}. It was chunked
              into {{count}} chunks, and the question was asked for each chunk.
              The results are:


              {{results}}




              You must now combine these answers into a single answer for this question: {{question}}
          outgoingConnections:
            - output->"Chat" rIyqK6UuYqLaIO23zgYM8/prompt
          visualData: 2459.7712182931887/307.24034013325036/300/31
        '[LbDS-ehJ0WcfC2CipqmJ-]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: output
          visualData: 3124.0951810571714/389.4792142403751/300/32
        '[Thf0b7P0ZBeb7ILxkuzVU]:text "Text"':
          data:
            text: You are a code file analyzer and question answerer. You are given the
              contents of a file of code, and you are given a question about the
              code. You directly answer the question given, with as much detail
              as possible. However, you do not output code unless you are
              explicitly asked to reply with some code. Otherwise, you reply
              with plain English only.
          outgoingConnections:
            - output->"Chat" jw7ZxX561t7Q_TYnVDEMW/systemPrompt
            - output->"Chat" rIyqK6UuYqLaIO23zgYM8/systemPrompt
          visualData: 1309.0936877506306/104.97158147670746/300/18
        '[ei3qRQVhPSrVddaO5bU-U]:extractObjectPath "Extract Object Path"':
          data:
            path: $.question
            usePathInput: false
          outgoingConnections:
            - match->"Text" BiOEV7VKd8ZRQz8vwM9Ng/question
            - match->"Text" nELWKzIKfsG5PysjF-cSg/question
          visualData: 611.4508579916097/714.4118633508281/250/21
        '[gpA-l85r9DUJnZux3JRjt]:text "Text"':
          data:
            text: |-
              Chunk {{index}}/{{count}} answer:

              {{answer}}
          isSplitRun: true
          outgoingConnections:
            - output->"Text" BiOEV7VKd8ZRQz8vwM9Ng/results
          visualData: 2049.0387588414633/427.4218704794111/300/30
        '[jw7ZxX561t7Q_TYnVDEMW]:chat "Chat"':
          data:
            cache: false
            enableFunctionUse: false
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-4-0613
            presencePenalty: 0
            stop: ""
            temperature: 0.3
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
            useUserInput: false
          isSplitRun: true
          outgoingConnections:
            - response->"Text" gpA-l85r9DUJnZux3JRjt/answer
          visualData: 1797.1530762644013/444.63882974839595/200/25
        '[lRV5SdMeGyEUVDkTN31ga]:readFile "Read File"':
          data:
            errorOnMissingFile: false
            path: ""
            usePathInput: true
          outgoingConnections:
            - content->"Chunk" 2338y8qNrzrbfDmzXJvhr/input
          visualData: 907.7722794413804/405.417269725349/250/12
        '[m9kiRPWzz-lraxQy_98B8]:extractObjectPath "Extract Object Path"':
          data:
            path: $.path
            usePathInput: false
          outgoingConnections:
            - match->"Read File" lRV5SdMeGyEUVDkTN31ga/path
            - match->"Text" BiOEV7VKd8ZRQz8vwM9Ng/file_name
            - match->"Text" nELWKzIKfsG5PysjF-cSg/file_name
          visualData: 613/365/250/2
        '[nELWKzIKfsG5PysjF-cSg]:text "Text"':
          data:
            text: >-
              Here is chunk {{index}}/{{count}} of a code file:


              ```

              // {{file_name}} ({{index}}/{{count}})


              {{contents}}

              ```



              I have this question about the file: {{question}}


              Answer to the best of your ability about this particular chunk of the file. If this chunk of the file does not answer the question, then state that this chunk of the file does not have a sufficient answer to the question.


              If the file does not exist, instruct the AI calling you that they should use READ_DIRECTORY and then call ASK_QUESTION_ABOUT_FILE again with the correct path.
          isSplitRun: true
          outgoingConnections:
            - output->"Chat" jw7ZxX561t7Q_TYnVDEMW/prompt
          visualData: 1446.7677235772446/307.9993433915212/300/24
        '[rIyqK6UuYqLaIO23zgYM8]:chat "Chat"':
          data:
            cache: false
            enableFunctionUse: false
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-4-0613
            presencePenalty: 0
            stop: ""
            temperature: 0.5
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
            useUserInput: false
          outgoingConnections:
            - response->"Graph Output" LbDS-ehJ0WcfC2CipqmJ-/value
          visualData: 2868.4447571236283/398.6248545653415/200/29
    9cRiigw77WY0G_wWDYp4Z:
      metadata:
        description: ""
        id: 9cRiigw77WY0G_wWDYp4Z
        name: RA - Exec Command
      nodes:
        '[1e5oUEYX0KeJz114J2SRQ]:subGraph "Subgraph"':
          data:
            graphId: g_lk18P0-BSirYNE2qGAa
          outgoingConnections:
            - output->"Coalesce" S-OKN_ugeBGnO3Ud4ks6N/input5
          visualData: 1509.9285357359627/645.4094053848103/300/23
        '[6-rZH-2eS8k3Ez0ygZgJn]:subGraph "Subgraph"':
          data:
            graphId: sgqPsMPVjV-hHvsAloa3y
          outgoingConnections:
            - output->"Coalesce" S-OKN_ugeBGnO3Ud4ks6N/input2
          visualData: 1508.6082012202514/212.33968423150543/300/17
        '[6eBKxYuyo2HBelLbBCBPY]:match "Match"':
          data:
            caseCount: 5
            cases:
              - LIST_FILES_IN_DIRECTORY
              - READ_FILE
              - ASK_QUESTION_ABOUT_FILE
              - REMEMBER_INFO
              - RECALL_INFO
          outgoingConnections:
            - case1->"If" HAqjF_Aqyiy8VjSaIdEOU/if
            - case2->"If" F7NBKP3NaBGQSztiVle1n/if
            - case3->"If" KtUEcTsgXn45Bx3A0VTeD/if
            - case4->"If" NNy8ZDMLOw45F_obP7Yfc/if
            - case5->"If" VGAj7Qwxgvf3Iz296cxfd/if
            - unmatched->"If" wJMY-M1cWCp5fp6z5WU98/if
          visualData: 952.9784825017156/161.27627193412414/300/9
        '[F7NBKP3NaBGQSztiVle1n]:if "If"':
          outgoingConnections:
            - output->"Subgraph" 6-rZH-2eS8k3Ez0ygZgJn/arguments
          visualData: 1350.055268436117/226.14086450213313/125/16
        '[HAqjF_Aqyiy8VjSaIdEOU]:if "If"':
          outgoingConnections:
            - output->"Subgraph" WoDTNQabmc1rZf2jFKGSK/arguments
          visualData: 1350.055268436117/89.17149806221684/125/11
        '[KtUEcTsgXn45Bx3A0VTeD]:if "If"':
          outgoingConnections:
            - output->"Subgraph" dzs2pgnFjSyTESOtah5Ct/arguments
          visualData: 1343.4535958575607/361.7898964263381/125/18
        '[NNy8ZDMLOw45F_obP7Yfc]:if "If"':
          outgoingConnections:
            - output->"Subgraph" xfdWAxd2JfuEbN1ZBgMC5/arguments
          visualData: 1343.4535958575607/513.2829425390786/125/20
        '[S-OKN_ugeBGnO3Ud4ks6N]:coalesce "Coalesce"':
          outgoingConnections:
            - output->"Prompt" eEXAsNDCNSOuZyEHA8EOH/results
          visualData: 2036.7420075047696/15.609841390522348/150/15
        '[SxQDo-a8rRHE1wic8UNx1]:graphInput "Graph Input"':
          data:
            dataType: object
            id: command
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Name" gQ9J8ptvSpdHjC0K_0Bk0/object
            - data->"Name" hy2vJwbBEwYKCjMs2OMTP/object
          visualData: 272/386/300/1
        '[VGAj7Qwxgvf3Iz296cxfd]:if "If"':
          outgoingConnections:
            - output->"Subgraph" 1e5oUEYX0KeJz114J2SRQ/arguments
          visualData: 1343.4535958575607/638.3692983375932/125/22
        '[WoDTNQabmc1rZf2jFKGSK]:subGraph "Subgraph"':
          data:
            graphId: lXUC_1uq9nRw3WNXCkpdB
          outgoingConnections:
            - output->"Coalesce" S-OKN_ugeBGnO3Ud4ks6N/input1
          visualData: 1507.9088279784255/73.58102304569259/300/12
        '[dzs2pgnFjSyTESOtah5Ct]:subGraph "Subgraph"':
          data:
            graphId: 7zpW_cdTXlsQbF5nNcgKP
          outgoingConnections:
            - output->"Coalesce" S-OKN_ugeBGnO3Ud4ks6N/input3
          visualData: 1515.2098737988078/357.57648095974787/300/19
        '[eEXAsNDCNSOuZyEHA8EOH]:prompt "Prompt"':
          data:
            enableFunctionCall: false
            name: SYSTEM INFORMATION
            promptText: |-
              Results for command {{command_name}}:

              """
              {{results}}
              """
            type: user
            useTypeInput: false
          outgoingConnections:
            - output->"Graph Output" z-12fYeNh0c6lZAGDQC5i/value
          visualData: 2254.428902438278/-109.11378005033785/250/24
        '[gQ9J8ptvSpdHjC0K_0Bk0]:extractObjectPath "Name"':
          data:
            path: |
              $.arguments
            usePathInput: false
          outgoingConnections:
            - match->"If" F7NBKP3NaBGQSztiVle1n/value
            - match->"If" HAqjF_Aqyiy8VjSaIdEOU/value
            - match->"If" KtUEcTsgXn45Bx3A0VTeD/value
            - match->"If" NNy8ZDMLOw45F_obP7Yfc/value
            - match->"If" VGAj7Qwxgvf3Iz296cxfd/value
          visualData: 719/481/153/5
        '[hy2vJwbBEwYKCjMs2OMTP]:extractObjectPath "Name"':
          data:
            path: |
              $.name
            usePathInput: false
          outgoingConnections:
            - match->"If" wJMY-M1cWCp5fp6z5WU98/value
            - match->"Match" 6eBKxYuyo2HBelLbBCBPY/input
            - match->"Prompt" eEXAsNDCNSOuZyEHA8EOH/command_name
          visualData: 719/248/153/4
        '[wJMY-M1cWCp5fp6z5WU98]:if "If"':
          outgoingConnections:
            - output->"Text" zAIWmNhfJj_h3xPx0kewP/command_name
          visualData: 1337.7457048662893/783.8203790315147/125/32
        '[xfdWAxd2JfuEbN1ZBgMC5]:subGraph "Subgraph"':
          data:
            graphId: pgbl6cyJ6kZWZwEb0o7rz
          outgoingConnections:
            - output->"Coalesce" S-OKN_ugeBGnO3Ud4ks6N/input4
          visualData: 1512.5692047673851/500.1726086565678/300/21
        '[z-12fYeNh0c6lZAGDQC5i]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: command_output
          visualData: 2622.320441954619/-41.23906807529085/300/27
        '[zAIWmNhfJj_h3xPx0kewP]:text "Text"':
          data:
            text: Unknown command {{command_name}}! Please refer to the system commands list
              to see all available commands.
          outgoingConnections:
            - output->"Coalesce" S-OKN_ugeBGnO3Ud4ks6N/input6
          visualData: 1501.6714910083379/787.9185236850659/300/31
    BCH2-JTaOfU7yrJ1GQRhL:
      metadata:
        description: ""
        id: BCH2-JTaOfU7yrJ1GQRhL
        name: Extract List Items
      nodes:
        '[0ciq8PeUozuVOAQ8D6lQ_]:ifElse "If/Else"':
          outgoingConnections:
            - output->"Graph Output" N-LYZkuLW9qvIFobDpRo5/value
          visualData: 1387/387/125/18
        '[FClS4P3cvRs8mZHieN9VO]:match "Match"':
          data:
            caseCount: 1
            cases:
              - \*
          outgoingConnections:
            - case1->"Extract Regex" uscIqNzkwiFK1zWdxydCj/input
            - case1->"If/Else" 0ciq8PeUozuVOAQ8D6lQ_/if
          visualData: 663/451/300/12
        '[JXxrnJnWCwYxJbnlgh_KM]:graphInput "Graph Input"':
          data:
            dataType: string
            id: text
          outgoingConnections:
            - data->"Match" FClS4P3cvRs8mZHieN9VO/input
          visualData: 288/323/300/8
        '[MGGvYxorqqDFpvGmFLWmw]:text "Text"':
          data:
            text: ""
          outgoingConnections:
            - output->"If/Else" 0ciq8PeUozuVOAQ8D6lQ_/false
          visualData: 688/648/154/16
        '[N-LYZkuLW9qvIFobDpRo5]:graphOutput "Graph Output"':
          data:
            dataType: string[]
            id: items
          visualData: 1625/402/300/14
        '[uscIqNzkwiFK1zWdxydCj]:extractRegex "Extract Regex"':
          data:
            errorOnFailed: true
            regex: \* (.+)
            useRegexInput: false
          outgoingConnections:
            - matches->"If/Else" 0ciq8PeUozuVOAQ8D6lQ_/true
          visualData: 1035/383/250/13
    DBSqt91CfAs1dewM-IhgT:
      metadata:
        description: ""
        id: DBSqt91CfAs1dewM-IhgT
        name: RA - Analyze Until Done
      nodes:
        '[-r_2kaeMS-ZYUXhsCObVV]:loopController "Loop Controller"':
          data:
            maxIterations: 100
          outgoingConnections:
            - break->"Extract Object Path" A1QAexk0N1yzfWytjwrlK/object
            - break->"Extract Object Path" Vd9CYlLETL0sAr2X73_lM/object
            - output1->"Subgraph" vLyGV7ovbUYwCpnVv_YTv/current_messages
          visualData: 1903.2274878891517/693.1500148714374/250/null
        '[2r-NqWWMhUJ2vG07yyGQe]:text "Text"':
          data:
            text: "false"
          outgoingConnections:
            - output->"If/Else" V50VnTrUvH3Xga1LRYKWS/false
          visualData: 3206.602702252145/1085.4509610314976/300/null
        '[A1QAexk0N1yzfWytjwrlK]:extractObjectPath "Extract Object Path"':
          data:
            path: $[1]
            usePathInput: false
          outgoingConnections:
            - match->"Graph Output" T5dFxOs_MHn0bLFC9aQDN/value
          visualData: 2262.08154047184/377.1058722189694/250/99
        '[EUuTrue5VkIy2ws195G0o]:match "Match"':
          data:
            caseCount: 1
            cases:
              - RECALL_INFO
          outgoingConnections:
            - case1->"If/Else" V50VnTrUvH3Xga1LRYKWS/if
          visualData: 3025.232574383944/603.9840477327783/300/95
        '[Stx1H2iniItB-FoT4f2bS]:extractYaml "Extract YAML"':
          data:
            objectPath: $.systemCommands[0].name
            rootPropertyName: systemCommands
          outgoingConnections:
            - output->"Match" EUuTrue5VkIy2ws195G0o/input
          visualData: 2683.4297003524184/613.8913774148516/250/94
        '[T5dFxOs_MHn0bLFC9aQDN]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: chat_output
          visualData: 2687.6193939503723/356.4100212762155/300/101
        '[V50VnTrUvH3Xga1LRYKWS]:ifElse "If/Else"':
          outgoingConnections:
            - output->"Loop Controller" -r_2kaeMS-ZYUXhsCObVV/continue
          visualData: 3563.9318877494343/567.0045981448056/125/97
        '[Va4d_NrMAW2V8LMdg5WIb]:text "Text"':
          data:
            text: "true"
          outgoingConnections:
            - output->"If/Else" V50VnTrUvH3Xga1LRYKWS/true
          visualData: 3208.1979218302577/870.0963179862564/300/null
        '[Vd9CYlLETL0sAr2X73_lM]:extractObjectPath "Extract Object Path"':
          data:
            path: $[0]
            usePathInput: false
          outgoingConnections:
            - match->"Graph Output" xCY6ItteekfA9ycKqnT1Q/value
          visualData: 2247.684285398584/163.36755714024667/250/98
        '[_fzwpRZw_bUBIul-X2mMs]:prompt "Prompt"':
          data:
            enableFunctionCall: false
            name: SYSTEM INFORMATION
            promptText: Next, talk about the next thing you will be analyzing, and run
              RECALL_INFO one or more times to see what you have already
              analyzed and remembered. Be as descriptive and verbose as possible
              in your topic argument, because the more information you give, the
              better the relevant memories can be identified.
            type: user
            useTypeInput: false
          visualData: 1650.9191658546888/358.25772586434255/250/93
        '[oW8bIRG1wJNWkXl-NS_qe]:prompt "Prompt"':
          data:
            enableFunctionCall: false
            name: SYSTEM INFORMATION
            promptText: >-
              What is the next thing you plan on doing? Run RECALL_INFO first
              before you run any other command.


              If you have already run RECALL_INFO for the next thing you plan on doing, and you do not have any memories for the task, you may run a command other than RECALL_INFO to further your memories.
            type: user
            useTypeInput: false
          outgoingConnections:
            - output->"Subgraph" vLyGV7ovbUYwCpnVv_YTv/prompt_message
          visualData: 1903.556072747556/496.03363823409995/250/92
        '[tSUsLaivP4rtiuLalg8Rn]:graphInput "Graph Input"':
          data:
            dataType: any
            id: current_messages
          outgoingConnections:
            - data->"Loop Controller" -r_2kaeMS-ZYUXhsCObVV/input1Default
          visualData: 1361.7278909749107/715.2333074499701/300/87
        '[vLyGV7ovbUYwCpnVv_YTv]:subGraph "Subgraph"':
          data:
            graphId: 49ADNSJiXKFRvgaRMhFvP
          outgoingConnections:
            - chat_output->"Extract YAML" Stx1H2iniItB-FoT4f2bS/input
            - chat_output->"Loop Controller" -r_2kaeMS-ZYUXhsCObVV/input2
            - new_messages->"Loop Controller" -r_2kaeMS-ZYUXhsCObVV/input1
          visualData: 2292.6967321450957/710.3927639039126/300/90
        '[xCY6ItteekfA9ycKqnT1Q]:graphOutput "Graph Output"':
          data:
            dataType: chat-message[]
            id: new_messages
          visualData: 2699.672402408892/135.11899467921754/300/100
    HRMMlTL5W-Wau-f7nLLEh:
      metadata:
        description: ""
        id: HRMMlTL5W-Wau-f7nLLEh
        name: RA - System Prompt
      nodes:
        '[fFj_MIJQSxY4S5DIBtnFk]:text "Text"':
          data:
            text: >-
              # You


              You are a source code analyzer. A user will give you a path to a repository. You have a set of system commands that you can then use to interact with the file system and other APIs in order to perform actions. You will go through and interatively analyze the repository, after each command you will update your analysis (either refining it as you have learned new information, or adding to your analysis in some way).


              You store your complete analysis using a memory with the ID "analysis".


              You frequently check your memory to see if you have already accomplished things.


              Before reading or asking questions about any files, you always run RECALL_INFO first to see if you have already done it.


              You always reply with a system command to interact with the System. However, you include text before your system command explaining your current thoughts.


              There may only be at most one `systemCommands` block in your responses.


              {{system_commands}}
          outgoingConnections:
            - output->"Graph Output" okr0iIB5gCjEG7zNOfDkP/value
          visualData: 578/287/300/null
        '[hV6TCfunJysRgAgjMK1nj]:subGraph "Subgraph"':
          data:
            graphId: cWbUqJZE2osNm42Q2ewC9
          outgoingConnections:
            - output->"Text" fFj_MIJQSxY4S5DIBtnFk/system_commands
          visualData: 179.6969610905414/450.1803932480233/300/53
        '[okr0iIB5gCjEG7zNOfDkP]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: system_prompt
          visualData: 1078/287/300/null
    HXjZhpWO0hluMiDY6pneE:
      metadata:
        description: ""
        id: HXjZhpWO0hluMiDY6pneE
        name: Digest File
      nodes:
        '[-c9ebiNnhmYedQN8Wo8hw]:chat "Chat"':
          data:
            cache: true
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0
            stop: ""
            temperature: 0
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          outgoingConnections:
            - response->"Text" IYT_PxYTFx5P4zMzhOqn_/name
            - response->"Text" IYT_PxYTFx5P4zMzhOqn_/responses
          visualData: 1785.4317276583695/362.6631476479146/200/52
        '[7VrEtnVAjfEeOfFVEBu_B]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: digest
          visualData: 3540.1001678580496/818.9951954575467/300/96
        '[BykrT115qLE_8iVjmBCAp]:text "Text"':
          data:
            text: Please tell me what the responsibility for this file is with respect to
              other files. Be extremely thorough in your response. Basically, if
              another person asked you "what is the purpose of this file? What
              does it do?", this would be your detailed response.
          outgoingConnections:
            - output->"Assemble Prompt" iNgsHuW1d-8xDv1A21bXK/message3
            - output->"Chat" _Gs7cTnnvcCHLxF8W5mFD/message3
          visualData: 971.7770959717168/1315.8146243045526/300/94
        '[CNa2GH7LRJIXnWGt366QH]:subGraph "Extract List Items"':
          data:
            graphId: BCH2-JTaOfU7yrJ1GQRhL
          outgoingConnections:
            - items->"Text" f6u3NUOg5S__5gSn8MXUF/exportedTypes
          visualData: 2392.746502255515/387.9603279891064/300/59
        '[Dl9zJxgRRBk7hQ43xWhZv]:text "Text"':
          data:
            text: >-
              

              Please give me a a list of the top level exported names/functions/etc from this part of the file. Put the items in bullet points.


              Example:


              * type Foo

              * type Bar
          outgoingConnections:
            - output->"Assemble Prompt" piQXWR9MhnC5iwPAK7zOC/message3
            - output->"Chat" -c9ebiNnhmYedQN8Wo8hw/message3
          visualData: 1037.1953261535937/389.80109239577234/300/91
        '[H7yY7sJF7zumxm1akfDD5]:text "Text"':
          data:
            text: |-
              Here is part {{index}}/{{count}} of a file named {{fileName}}:

              ```
              {{content}}
              ```
          outgoingConnections:
            - output->"Assemble Prompt" ZBqKPDo-QOpXD-Xo0XXpD/message2
            - output->"Assemble Prompt" iNgsHuW1d-8xDv1A21bXK/message2
            - output->"Assemble Prompt" piQXWR9MhnC5iwPAK7zOC/message2
            - output->"Chat" -c9ebiNnhmYedQN8Wo8hw/message2
            - output->"Chat" _Gs7cTnnvcCHLxF8W5mFD/message2
            - output->"Chat" jB5sIeojQRHXqNPoMQ3hD/message2
          visualData: 921.042358937134/607.8022923490608/300/50
        '[IYT_PxYTFx5P4zMzhOqn_]:text "Text"':
          data:
            text: "{{responses}}"
          outgoingConnections:
            - output->"Extract List Items" CNa2GH7LRJIXnWGt366QH/text
          visualData: 2045.0325311729325/391.53579298738487/300/57
        '[MA5Uv_ITTzx9NQV08ugic]:text "Text"':
          data:
            text: "{{responses}}"
          outgoingConnections:
            - output->"Text" f6u3NUOg5S__5gSn8MXUF/imports
          visualData: 2011.055065642513/922.81832877346/300/80
        '[SrMnr7J2d7Buwtd0vl8OF]:chunk "Chunk"':
          data:
            model: gpt-3.5-turbo
            numTokensPerChunk: 2048
            overlap: 0
            useModelInput: false
          outgoingConnections:
            - chunks->"Text" H7yY7sJF7zumxm1akfDD5/content
            - count->"Text" H7yY7sJF7zumxm1akfDD5/count
            - indexes->"Text" H7yY7sJF7zumxm1akfDD5/index
          visualData: 513.6527622586358/541.0860856359533/200/49
        '[ZBqKPDo-QOpXD-Xo0XXpD]:assemblePrompt "Assemble Prompt"':
          outgoingConnections:
            - prompt->"Chat" jB5sIeojQRHXqNPoMQ3hD/prompt
          visualData: 1434.5456573857782/944.3831259770584/250/93
        '[ZghD25qGelXbZLU6VLwLm]:prompt "Prompt"':
          data:
            promptText: You are a sophisticated AI tool for extracting and analyzing code
              for the purposes of summarizing/digesting a code file. Given a
              request, you give a direct response.
            type: system
            useTypeInput: false
          outgoingConnections:
            - output->"Assemble Prompt" ZBqKPDo-QOpXD-Xo0XXpD/message1
            - output->"Assemble Prompt" iNgsHuW1d-8xDv1A21bXK/message1
            - output->"Assemble Prompt" piQXWR9MhnC5iwPAK7zOC/message1
            - output->"Chat" -c9ebiNnhmYedQN8Wo8hw/message1
            - output->"Chat" _Gs7cTnnvcCHLxF8W5mFD/message1
            - output->"Chat" jB5sIeojQRHXqNPoMQ3hD/message1
          visualData: 331.1046122293854/209.09150971792653/null/42
        '[_Gs7cTnnvcCHLxF8W5mFD]:chat "Chat"':
          data:
            cache: true
            frequencyPenalty: 0.2
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0.2
            stop: ""
            temperature: 0
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          outgoingConnections:
            - response->"Text" qP77T0iicnT05aOyXu4bl/responses
          visualData: 1787.0996057501923/1218.8364162908351/200/73
        '[cQlhtmjsV2OU-_F9vbLni]:text "Text"':
          data:
            text: "{{fileName}}"
          outgoingConnections:
            - output->"Text" f6u3NUOg5S__5gSn8MXUF/fileName
          visualData: 2237.7406238561566/643.7676374637542/141.47817312027655/68
        '[f6u3NUOg5S__5gSn8MXUF]:text "Text"':
          data:
            text: |-
              Here is a summary of {{fileName}}:

              Exported Types:
              {{exportedTypes}}

              Imports From:
              {{imports}}

              Summary:

              """
              {{summary}}
              ""
          outgoingConnections:
            - output->"Graph Output" 7VrEtnVAjfEeOfFVEBu_B/value
          visualData: 3159.6649042897557/663.832260816221/300/90
        '[iNgsHuW1d-8xDv1A21bXK]:assemblePrompt "Assemble Prompt"':
          outgoingConnections:
            - prompt->"Chat" _Gs7cTnnvcCHLxF8W5mFD/prompt
          visualData: 1427.8561981049036/1298.9244678634084/250/95
        '[jB5sIeojQRHXqNPoMQ3hD]:chat "Chat"':
          data:
            cache: true
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0
            stop: ""
            temperature: 0
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          outgoingConnections:
            - response->"Text" MA5Uv_ITTzx9NQV08ugic/responses
          visualData: 1785.2088940836688/871.453829230769/200/62
        '[pVOBgjtUCeoRf7O_aUK5z]:graphInput "Graph Input"':
          data:
            dataType: string
            defaultValue: Nodes.ts
            id: file_name
          outgoingConnections:
            - data->"Text" H7yY7sJF7zumxm1akfDD5/fileName
            - data->"Text" cQlhtmjsV2OU-_F9vbLni/fileName
          visualData: 467.8341151842628/843.852274140362/300/77
        '[pZFkwyQ1DNAh158xMFaOr]:text "Text"':
          data:
            text: >-
              If there are any relative imports, please list the files that are
              being imported from. If there are no relative imports, say NO
              IMPORTS.


              Example:


              * ./File1.ts

              * ../File2.ts


              Another example:


              NO IMPORTS
          outgoingConnections:
            - output->"Assemble Prompt" ZBqKPDo-QOpXD-Xo0XXpD/message3
            - output->"Chat" jB5sIeojQRHXqNPoMQ3hD/message3
          visualData: 985.2658327776197/899.6643074227177/300/92
        '[piQXWR9MhnC5iwPAK7zOC]:assemblePrompt "Assemble Prompt"':
          outgoingConnections:
            - prompt->"Chat" -c9ebiNnhmYedQN8Wo8hw/prompt
          visualData: 1470.2227735504423/275.4371978896052/250/null
        '[qP77T0iicnT05aOyXu4bl]:text "Text"':
          data:
            text: "{{responses}}"
          outgoingConnections:
            - output->"Text" f6u3NUOg5S__5gSn8MXUF/summary
          visualData: 2068.5571459636226/1283.7209368325432/300/74
        '[wGuXGa_VR2yGn5F3Gm-r7]:graphInput "Graph Input"':
          data:
            dataType: string
            id: file_contents
            useDefaultValueInput: true
          outgoingConnections:
            - data->"Chunk" SrMnr7J2d7Buwtd0vl8OF/input
          visualData: 141.6021328645315/556.4574325729797/300/78
        '[yQD0z-eBeqo0Hmb0yhZ05]:text "Text"':
          data:
            text: >
              import { ChartNode } from './NodeBase.js';

              import { UserInputNode, UserInputNodeImpl } from './nodes/UserInputNode.js';

              import { NodeImpl } from './NodeImpl.js';

              import { TextNode, TextNodeImpl } from './nodes/TextNode.js';

              import { ChatNode, ChatNodeImpl } from './nodes/ChatNode.js';

              import { PromptNode, PromptNodeImpl } from './nodes/PromptNode.js';

              import { match } from 'ts-pattern';

              import { ExtractRegexNode, ExtractRegexNodeImpl } from './nodes/ExtractRegexNode.js';

              import { CodeNode, CodeNodeImpl } from './nodes/CodeNode.js';

              import { MatchNode, MatchNodeImpl } from './nodes/MatchNode.js';

              import { IfNode, IfNodeImpl } from './nodes/IfNode.js';

              import { ReadDirectoryNode, ReadDirectoryNodeImpl } from './nodes/ReadDirectoryNode.js';

              import { ReadFileNode, ReadFileNodeImpl } from './nodes/ReadFileNode.js';

              import { IfElseNode, IfElseNodeImpl } from './nodes/IfElseNode.js';

              import { ChunkNode, ChunkNodeImpl } from './nodes/ChunkNode.js';

              import { GraphInputNode, GraphInputNodeImpl } from './nodes/GraphInputNode.js';

              import { GraphOutputNode, GraphOutputNodeImpl } from './nodes/GraphOutputNode.js';

              import { SubGraphNode, SubGraphNodeImpl } from './nodes/SubGraphNode.js';

              import { ArrayNode, ArrayNodeImpl } from './nodes/ArrayNode.js';

              import { ExtractJsonNode, ExtractJsonNodeImpl } from './nodes/ExtractJsonNode.js';

              import { AssemblePromptNode, AssemblePromptNodeImpl } from './nodes/AssemblePromptNode.js';

              import { LoopControllerNode, LoopControllerNodeImpl } from './nodes/LoopControllerNode.js';


              export type Nodes =
                | UserInputNode
                | TextNode
                | ChatNode
                | PromptNode
                | ExtractRegexNode
                | CodeNode
                | MatchNode
                | IfNode
                | ReadDirectoryNode
                | ReadFileNode
                | IfElseNode
                | ChunkNode
                | GraphInputNode
                | GraphOutputNode
                | SubGraphNode
                | ArrayNode
                | ExtractJsonNode
                | AssemblePromptNode
                | LoopControllerNode;

              export * from './nodes/UserInputNode';

              export * from './nodes/TextNode';

              export * from './nodes/ChatNode';

              export * from './nodes/PromptNode';

              export * from './nodes/ExtractRegexNode';

              export * from './nodes/CodeNode';

              export * from './nodes/MatchNode';

              export * from './nodes/IfNode';

              export * from './nodes/ReadDirectoryNode';

              export * from './nodes/ReadFileNode';

              export * from './nodes/IfElseNode';

              export * from './nodes/ChunkNode';

              export * from './nodes/GraphInputNode';

              export * from './nodes/GraphOutputNode';

              export * from './nodes/SubGraphNode';

              export * from './nodes/ArrayNode';

              export * from './nodes/ExtractJsonNode';

              export * from './nodes/AssemblePromptNode';

              export * from './nodes/LoopControllerNode';


              export type NodeType = Nodes['type'];


              export const createNodeInstance = <T extends Nodes>(node: T): NodeImpl<ChartNode> => {
                return match(node as Nodes)
                  .with({ type: 'userInput' }, (node) => new UserInputNodeImpl(node))
                  .with({ type: 'text' }, (node) => new TextNodeImpl(node))
                  .with({ type: 'chat' }, (node) => new ChatNodeImpl(node))
                  .with({ type: 'prompt' }, (node) => new PromptNodeImpl(node))
                  .with({ type: 'extractRegex' }, (node) => new ExtractRegexNodeImpl(node))
                  .with({ type: 'code' }, (node) => new CodeNodeImpl(node))
                  .with({ type: 'match' }, (node) => new MatchNodeImpl(node))
                  .with({ type: 'if' }, (node) => new IfNodeImpl(node))
                  .with({ type: 'readDirectory' }, (node) => new ReadDirectoryNodeImpl(node))
                  .with({ type: 'readFile' }, (node) => new ReadFileNodeImpl(node))
                  .with({ type: 'ifElse' }, (node) => new IfElseNodeImpl(node))
                  .with({ type: 'chunk' }, (node) => new ChunkNodeImpl(node))
                  .with({ type: 'graphInput' }, (node) => new GraphInputNodeImpl(node))
                  .with({ type: 'graphOutput' }, (node) => new GraphOutputNodeImpl(node))
                  .with({ type: 'subGraph' }, (node) => new SubGraphNodeImpl(node))
                  .with({ type: 'array' }, (node) => new ArrayNodeImpl(node))
                  .with({ type: 'extractJson' }, (node) => new ExtractJsonNodeImpl(node))
                  .with({ type: 'assemblePrompt' }, (node) => new AssemblePromptNodeImpl(node))
                  .with({ type: 'loopController' }, (node) => new LoopControllerNodeImpl(node))
                  .exhaustive();
              };


              export function createUnknownNodeInstance(node: ChartNode): NodeImpl<ChartNode> {
                return createNodeInstance(node as Nodes);
              }


              export function nodeFactory(type: NodeType): Nodes {
                return match(type)
                  .with('userInput', () => UserInputNodeImpl.create())
                  .with('text', () => TextNodeImpl.create())
                  .with('chat', () => ChatNodeImpl.create())
                  .with('prompt', () => PromptNodeImpl.create())
                  .with('extractRegex', () => ExtractRegexNodeImpl.create())
                  .with('code', () => CodeNodeImpl.create())
                  .with('match', () => MatchNodeImpl.create())
                  .with('if', () => IfNodeImpl.create())
                  .with('readDirectory', () => ReadDirectoryNodeImpl.create())
                  .with('readFile', () => ReadFileNodeImpl.create())
                  .with('ifElse', () => IfElseNodeImpl.create())
                  .with('chunk', () => ChunkNodeImpl.create())
                  .with('graphInput', () => GraphInputNodeImpl.create())
                  .with('graphOutput', () => GraphOutputNodeImpl.create())
                  .with('subGraph', () => SubGraphNodeImpl.create())
                  .with('array', () => ArrayNodeImpl.create())
                  .with('extractJson', () => ExtractJsonNodeImpl.create())
                  .with('assemblePrompt', () => AssemblePromptNodeImpl.create())
                  .with('loopController', () => LoopControllerNodeImpl.create())
                  .exhaustive();
              }


              export const nodeDisplayName: Record<NodeType, string> = {
                userInput: 'User Input',
                text: 'Text',
                chat: 'Chat',
                prompt: 'Prompt',
                assemblePrompt: 'Assemble Prompt',
                extractRegex: 'Extract With Regex',
                extractJson: 'Extract JSON',
                code: 'Code',
                match: 'Match',
                if: 'If',
                ifElse: 'If/Else',
                loopController: 'Loop Controller',
                readDirectory: 'Read Directory',
                readFile: 'Read File',
                chunk: 'Chunk',
                graphInput: 'Graph Input',
                graphOutput: 'Graph Output',
                subGraph: 'Subgraph',
                array: 'Array',
              };
          outgoingConnections:
            - output->"Graph Input" wGuXGa_VR2yGn5F3Gm-r7/default
          visualData: -252.53117819207165/401.97028781741056/300/97
    JcFUPKbbvOvBQYdvItenL:
      metadata:
        description: ""
        id: JcFUPKbbvOvBQYdvItenL
        name: Get Rivet File By File Name
      nodes:
        '[3K-EbZswm_EjMm7YvDp-8]:extractRegex "Extract Regex"':
          data:
            errorOnFailed: false
            regex: ([a-zA-Z]+)
            useRegexInput: true
          outgoingConnections:
            - output1->"Extract Regex" dC9Q17Nml5jtpt-VrG2E6/input
            - output1->"Text" pL1gfs7YyNnriOtL8t129/path
          visualData: -260.64747296379056/576.2537087926187/250/29
        '[6AXzyyVMSyqo6LN36roAj]:text "Text"':
          data:
            text: (.*{{value}}.*)
          outgoingConnections:
            - output->"Extract Regex" 3K-EbZswm_EjMm7YvDp-8/regex
          visualData: -622.3074425333289/708.7566296572189/300/26
        '[8WYEygiw4rtsog1_vAhUN]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: file_name
          visualData: 510/665/300/84
        '[DPHvuC_nC6P9cHeMx6AfY]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: abs_path
          visualData: 498/257/300/85
        '[IJx_hhz0xYQMQ6LKp58Ut]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: file_contents
          visualData: 518/453/300/82
        '[XhzNj4pzQ2LB31xvUOa38]:readFile "Read File"':
          data:
            errorOnMissingFile: false
            path: ""
            usePathInput: true
          outgoingConnections:
            - content->"Graph Output" IJx_hhz0xYQMQ6LKp58Ut/value
          visualData: 78.53696254114706/589.5330651481295/250/30
        '[dC9Q17Nml5jtpt-VrG2E6]:extractRegex "Extract Regex"':
          data:
            errorOnFailed: false
            regex: ([^/]+$)
            useRegexInput: false
          outgoingConnections:
            - output1->"Graph Output" 8WYEygiw4rtsog1_vAhUN/value
          visualData: 75.95255224213457/757.5597475716329/250/34
        '[hnMBT-lVJPf62eBijBv23]:subGraph "List Rivet Files"':
          data:
            graphId: d6Pgmz7n8qvXkaNF-2e9P
          outgoingConnections:
            - files->"Text" o49i8FBO13BUTSJ9VVpUp/paths
            - root_path->"Text" pL1gfs7YyNnriOtL8t129/root
          visualData: -974/509/300/80
        '[o49i8FBO13BUTSJ9VVpUp]:text "Text"':
          data:
            text: "{{paths}}"
          outgoingConnections:
            - output->"Extract Regex" 3K-EbZswm_EjMm7YvDp-8/input
          visualData: -619.508171986833/504.0287905543141/300/28
        '[pL1gfs7YyNnriOtL8t129]:text "Text"':
          data:
            text: "{{root}}/{{path}}"
          outgoingConnections:
            - output->"Graph Output" DPHvuC_nC6P9cHeMx6AfY/value
            - output->"Read File" XhzNj4pzQ2LB31xvUOa38/path
          visualData: -91.24293197539083/370.6588599708293/300/78
        '[pS3t0dhKo4pndES52TC25]:graphInput "Graph Input"':
          data:
            dataType: string
            defaultValue: GraphProcessor.ts
            id: fileMatch
          outgoingConnections:
            - data->"Text" 6AXzyyVMSyqo6LN36roAj/value
          visualData: -973.2451596049881/711.7486797972081/300/24
    KVOCarbcccMX-wOaypQ7L:
      metadata:
        description: ""
        id: KVOCarbcccMX-wOaypQ7L
        name: Ask User Questions
      nodes:
        '[0EuQ8ONijZa5voMTZGTps]:prompt "Prompt"':
          data:
            promptText: >-
              I have the following question or request:


              """

              {{request}}

              """


              Please give a bulleted list of questions. You may ask anywhere from zero to five questions. Try to only ask a question if it is necessary for your fulfillment of the request.


              Here is an example of an answer:


              Questions:

              * Question 1?

              * Question 2?

              * Question 3?


              Another example (if you have enough context):


              Questions: NONE
            type: user
            useTypeInput: false
          outgoingConnections:
            - output->"Chat" G4wKhESWcRoSKT7DSsNdy/message1
          visualData: -193.28978127029873/304.7484539029439/null/16
        '[1Qj53m9VECmnmTWByVo_d]:userInput "User Input"':
          data:
            prompt: This is an example question?
            useInput: true
          outgoingConnections:
            - questionsAndAnswers->"Text" vpmf3hIf-86GRlg3Bg-Lu/qa
          visualData: 948/385/250/5
        '[2QcutpULV4uH0CkPgA3ti]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: output
          visualData: 1634/380/300/8
        '[G4wKhESWcRoSKT7DSsNdy]:chat "Chat"':
          data:
            cache: false
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0
            stop: ""
            temperature: 0.5
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          outgoingConnections:
            - response->"Extract List Items" XrdweoN7N1ectKzjP3WHS/text
          visualData: 339.06620729836334/457.8536805742646/200/17
        '[XrdweoN7N1ectKzjP3WHS]:subGraph "Extract List Items"':
          data:
            graphId: BCH2-JTaOfU7yrJ1GQRhL
          outgoingConnections:
            - items->"User Input" 1Qj53m9VECmnmTWByVo_d/questions
          visualData: 597/385/300/3
        '[glCn_a3AsNTs25UyCMCiG]:graphInput "Graph Input"':
          data:
            dataType: string
            id: AI Prompt
          outgoingConnections:
            - data->"Prompt" 0EuQ8ONijZa5voMTZGTps/request
          visualData: -560.0248699650894/345.73225810806554/300/15
        '[vpmf3hIf-86GRlg3Bg-Lu]:text "Text"':
          data:
            text: "{{qa}}"
          outgoingConnections:
            - output->"Graph Output" 2QcutpULV4uH0CkPgA3ti/value
          visualData: 1265/407/300/7
    LOt0W_XCiFbuqbRlbF1oM:
      metadata:
        description: ""
        id: LOt0W_XCiFbuqbRlbF1oM
        name: Split and summarize file
      nodes:
        '[67RewDBJlQUInF8WV3-GB]:text "Text"':
          data:
            text: >
              import { max, range, uniqBy } from 'lodash-es';

              import { ControlFlowExcluded } from '../utils/symbols.js';

              import { DataValue, ArrayDataValue, AnyDataValue, StringDataValue, expectType, ScalarDataValue } from './DataValue.js';

              import { ChartNode, NodeConnection, NodeId, NodeInputDefinition, NodeOutputDefinition, PortId } from './NodeBase.js';

              import { NodeGraph } from './NodeGraph.js';

              import { NodeImpl, ProcessContext } from './NodeImpl.js';

              import { Nodes, createNodeInstance } from './Nodes.js';

              import { UserInputNode, UserInputNodeImpl } from './nodes/UserInputNode.js';


              export type NodeResults = Map<string, Record<PortId, DataValue>>;


              export type ProcessEvents = {
                onNodeStart?: (node: ChartNode, inputs: Record<PortId, DataValue>) => void;
                onNodeFinish?: (node: ChartNode, result: Record<PortId, DataValue>) => void;
                onNodeError?: (node: ChartNode, error: Error) => void;
                onNodeExcluded?: (node: ChartNode) => void;
                onUserInput?: (
                  userInputNodes: UserInputNode[],
                  inputs: Record<PortId, DataValue>[],
                ) => Promise<ArrayDataValue<StringDataValue>[]>;
                onPartialOutputs?: (node: ChartNode, outputs: Record<PortId, DataValue>, index: number) => void;
              };


              export class GraphProcessor {
                #graph: NodeGraph;
                #nodeInstances: Record<NodeId, NodeImpl<ChartNode>>;
                #connections: Record<NodeId, NodeConnection[]>;
                #definitions: Record<NodeId, { inputs: NodeInputDefinition[]; outputs: NodeOutputDefinition[] }>;

                constructor(graph: NodeGraph) {
                  this.#graph = graph;
                  this.#nodeInstances = {};
                  this.#connections = {};

                  // Create node instances and store them in a lookup table
                  for (const node of this.#graph.nodes) {
                    this.#nodeInstances[node.id] = createNodeInstance(node as Nodes);
                  }

                  // Store connections in a lookup table
                  for (const conn of this.#graph.connections) {
                    if (!this.#connections[conn.inputNodeId]) {
                      this.#connections[conn.inputNodeId] = [];
                    }
                    if (!this.#connections[conn.outputNodeId]) {
                      this.#connections[conn.outputNodeId] = [];
                    }
                    this.#connections[conn.inputNodeId]!.push(conn);
                    this.#connections[conn.outputNodeId]!.push(conn);
                  }

                  // Store input and output definitions in a lookup table
                  this.#definitions = {};
                  for (const node of this.#graph.nodes) {
                    this.#definitions[node.id] = {
                      inputs: this.#nodeInstances[node.id]!.getInputDefinitions(this.#connections[node.id]!),
                      outputs: this.#nodeInstances[node.id]!.getOutputDefinitions(this.#connections[node.id]!),
                    };
                  }
                }

                #nodeIsReady(node: ChartNode, visitedNodes: Set<unknown>, depth = 0): boolean {
                  return this.#allInputsVisited(node, visitedNodes);
                }

                #allInputsVisited(node: ChartNode, visitedNodes: Set<unknown>, depth = 0): boolean {
                  const connections = this.#connections[node.id];
                  return (
                    this.#definitions[node.id]!.inputs.every((input) => {
                      const connectionToInput = connections?.find(
                        (conn) => conn.inputId === input.id && conn.inputNodeId === node.id,
                      );

                      if (!input.required && !connectionToInput) {
                        return true;
                      }

                      if (!connectionToInput) {
                        return false;
                      }

                      return visitedNodes.has(connectionToInput.outputNodeId);
                    }) || this.#definitions[node.id]!.inputs.length === 0
                  );
                }

                async processGraph(context: ProcessContext, events: ProcessEvents = {}): Promise<Record<string, DataValue>> {
                  const outputNodes = this.#graph.nodes.filter((node) => this.#definitions[node.id]!.outputs.length === 0);

                  const nodeResults: NodeResults = new Map();

                  // Process nodes in topological order
                  const nodesToProcess = [...this.#graph.nodes];
                  const visitedNodes = new Set();

                  while (nodesToProcess.length > 0) {
                    const readyNodes = nodesToProcess.filter((node) => this.#nodeIsReady(node, visitedNodes));

                    if (readyNodes.length === 0) {
                      for (const erroredNode of nodesToProcess) {
                        events.onNodeError?.(
                          erroredNode,
                          new Error('There might be a cycle in the graph or an issue with input dependencies.'),
                        );
                      }
                      throw new Error('There might be a cycle in the graph or an issue with input dependencies.');
                    }

                    const userInputNodes = readyNodes.filter((node) => node.type === 'userInput') as UserInputNode[];
                    if (userInputNodes.length > 0 && events.onUserInput) {
                      try {
                        const validUserInputNodes: UserInputNode[] = [];
                        const userInputInputValues: Record<PortId, DataValue>[] = [];

                        for (const node of userInputNodes) {
                          const inputValues = this.#getInputValuesForNode(node, nodeResults);
                          if (this.#excludedDueToControlFlow(node, nodeResults, inputValues, events, visitedNodes)) {
                            continue;
                          }
                          validUserInputNodes.push(node);
                          userInputInputValues.push(inputValues);
                          events.onNodeStart?.(node, inputValues);
                        }

                        if (validUserInputNodes.length > 0) {
                          const userInputResults = await events.onUserInput(validUserInputNodes, userInputInputValues);
                          userInputResults.forEach((result, index) => {
                            const node = validUserInputNodes[index]!;
                            const outputValues = (this.#nodeInstances[node.id] as UserInputNodeImpl).getOutputValuesFromUserInput(
                              userInputInputValues[index]!,
                              result,
                            );
                            nodeResults.set(node.id, outputValues);
                            visitedNodes.add(node.id);
                            nodesToProcess.splice(nodesToProcess.indexOf(node), 1);
                            events.onNodeFinish?.(node, outputValues);
                          });
                          continue;
                        }
                      } catch (error) {
                        for (const node of userInputNodes) {
                          events.onNodeError?.(node, error as Error);
                        }
                        throw error;
                      }
                    }

                    await Promise.allSettled(
                      readyNodes.map(async (node) => {
                        await this.#processNode(node as Nodes, nodeResults, context, events, visitedNodes, nodesToProcess);
                      }),
                    );
                  }

                  // Collect output values
                  const outputValues = outputNodes.reduce((values, node) => {
                    values[node.id] = nodeResults.get(node.id);
                    return values;
                  }, {} as Record<string, any>);

                  return outputValues;
                }

                async #processNode(
                  node: Nodes,
                  nodeResults: NodeResults,
                  context: ProcessContext,
                  events: ProcessEvents,
                  visitedNodes: Set<unknown>,
                  nodesToProcess: ChartNode[],
                ) {
                  nodesToProcess.splice(nodesToProcess.indexOf(node), 1);

                  if (node.isSplitRun) {
                    await this.#processSplitRunNode(node, nodeResults, context, events, visitedNodes);
                  } else {
                    await this.#processNormalNode(node, nodeResults, context, events, visitedNodes);
                  }
                }

                async #processSplitRunNode(
                  node: ChartNode,
                  nodeResults: NodeResults,
                  context: ProcessContext,
                  events: ProcessEvents,
                  visitedNodes: Set<unknown>,
                ) {
                  const inputValues = this.#getInputValuesForNode(node, nodeResults);

                  if (this.#excludedDueToControlFlow(node, nodeResults, inputValues, events, visitedNodes)) {
                    return;
                  }

                  const splittingAmount = Math.min(
                    max(Object.values(inputValues).map((value) => (Array.isArray(value.value) ? value.value.length : 1))) ?? 1,
                    node.splitRunMax ?? 10,
                  );

                  events.onNodeStart?.(node, inputValues);

                  try {
                    const results: Record<PortId, DataValue>[] = [];

                    await Promise.all(
                      range(0, splittingAmount).map(async (i) => {
                        const inputs: Record<PortId, DataValue> = Object.fromEntries(
                          Object.entries(inputValues).map(([port, value]): [PortId, DataValue] => {
                            if (value.type.endsWith('[]')) {
                              const newType = value.type.slice(0, -2) as DataValue['type'];
                              const newValue: unknown = (value.value as unknown[])[i] ?? undefined;
                              return [port as PortId, { type: newType, value: newValue as any }];
                            } else {
                              return [port as PortId, value];
                            }
                          }),
                        );

                        try {
                          const output = await this.#processNodeWithInputData(node, context, inputs, i, events.onPartialOutputs);
                          results.push(output);
                        } catch (error) {
                          const errorInstance =
                            typeof error === 'object' && error instanceof Error
                              ? error
                              : new Error(error != null ? error.toString() : 'Unknown error');
                          events.onNodeError?.(node, errorInstance);
                          throw error;
                        }
                      }),
                    );

                    // Combine the parallel results into the final output

                    // Turn a Record<PortId, DataValue[]> into a Record<PortId, AnyArrayDataValue>
                    const aggregateResults = results.reduce((acc, result) => {
                      for (const [portId, value] of Object.entries(result)) {
                        acc[portId as PortId] ??= { type: (value.type + '[]') as DataValue['type'], value: [] } as DataValue;
                        (acc[portId as PortId] as ArrayDataValue<AnyDataValue>).value.push(value.value);
                      }
                      return acc;
                    }, {} as Record<PortId, DataValue>);

                    nodeResults.set(node.id, aggregateResults);
                    visitedNodes.add(node.id);
                    events.onNodeFinish?.(node, aggregateResults);
                  } catch (error) {
                    const errorInstance =
                      typeof error === 'object' && error instanceof Error
                        ? error
                        : new Error(error != null ? error.toString() : 'Unknown error');
                    events.onNodeError?.(node, errorInstance);
                    console.error(error);
                    throw error;
                  }
                }

                async #processNormalNode(
                  node: ChartNode,
                  nodeResults: NodeResults,
                  context: ProcessContext,
                  events: ProcessEvents,
                  visitedNodes: Set<unknown>,
                ) {
                  const inputValues = this.#getInputValuesForNode(node, nodeResults);

                  if (this.#excludedDueToControlFlow(node, nodeResults, inputValues, events, visitedNodes)) {
                    return;
                  }

                  events.onNodeStart?.(node, inputValues);

                  try {
                    const outputValues = await this.#processNodeWithInputData(node, context, inputValues, 0, events.onPartialOutputs);

                    nodeResults.set(node.id, outputValues);
                    visitedNodes.add(node.id);
                    events.onNodeFinish?.(node, outputValues);
                  } catch (error) {
                    const errorInstance =
                      typeof error === 'object' && error instanceof Error
                        ? error
                        : new Error(error != null ? error.toString() : 'Unknown error');
                    events.onNodeError?.(node, errorInstance);
                    throw error;
                  }
                }

                async #processNodeWithInputData(
                  node: ChartNode,
                  context: ProcessContext,
                  inputValues: Record<PortId, DataValue>,
                  index: number,
                  onPartialOutputs?: (node: ChartNode, partialOutputs: Record<PortId, DataValue>, index: number) => void,
                ) {
                  return await this.#nodeInstances[node.id]!.process(inputValues, context, (partialOutputs) =>
                    onPartialOutputs?.(node, partialOutputs, index),
                  );
                }

                #excludedDueToControlFlow(
                  node: ChartNode,
                  nodeResults: NodeResults,
                  inputValues: Record<PortId, DataValue>,
                  { onNodeExcluded }: { onNodeExcluded?: (node: ChartNode) => void },
                  visitedNodes: Set<unknown>,
                ) {
                  const inputValuesList = Object.values(inputValues);
                  const inputIsExcludedValue =
                    inputValuesList.length > 0 && inputValuesList.some((value) => value?.type === 'control-flow-excluded');

                  const inputConnections = this.#connections[node.id]?.filter((conn) => conn.inputNodeId === node.id) ?? [];
                  const outputNodes = inputConnections
                    .map((conn) => this.#graph.nodes.find((n) => n.id === conn.outputNodeId))
                    .filter((n) => n) as ChartNode[];

                  const anyOutputIsExcludedValue =
                    outputNodes.length > 0 &&
                    outputNodes.some((outputNode) => {
                      const outputValues = nodeResults.get(outputNode.id) ?? {};
                      if (outputValues[ControlFlowExcluded as unknown as PortId]) {
                        return true;
                      }
                      return false;
                    });

                  const allowedToConsumedExcludedValue = node.type === 'if' || node.type === 'ifElse';

                  if ((inputIsExcludedValue || anyOutputIsExcludedValue) && !allowedToConsumedExcludedValue) {
                    onNodeExcluded?.(node);
                    visitedNodes.add(node.id);
                    nodeResults.set(node.id, {
                      [ControlFlowExcluded as unknown as PortId]: { type: 'control-flow-excluded', value: undefined },
                    });
                    return true;
                  }

                  return false;
                }

                #getInputValuesForNode(node: ChartNode, nodeResults: NodeResults): Record<PortId, DataValue> {
                  const connections = this.#connections[node.id];
                  return this.#definitions[node.id]!.inputs.reduce((values, input) => {
                    if (!connections) {
                      return values;
                    }
                    const connection = connections.find((conn) => conn.inputId === input.id && conn.inputNodeId === node.id);
                    if (connection) {
                      const outputNode = this.#nodeInstances[connection.outputNodeId]!.chartNode;
                      const outputResult = nodeResults.get(outputNode.id)?.[connection.outputId];

                      values[input.id] = outputResult;
                    }
                    return values;
                  }, {} as Record<string, any>);
                }
              }
          outgoingConnections:
            - output->"Chunk" RtTfDTcXP3eigkIWeAhh-/input
          visualData: 437.4469649520634/314.3452826722073/300/null
        '[F_dKF3vDhO7Cr1c8mvaKP]:chat "Chat"':
          data:
            cache: true
            frequencyPenalty: 0
            maxTokens: 4096
            model: gpt-3.5-turbo
            presencePenalty: 0
            temperature: 0
            top_p: 1
            useMaxTokensInput: false
            useModelInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          outgoingConnections:
            - response->"Text" rBQqW1q3Qp07IV6R66kMF/summaries
            - response->"Text" rBQqW1q3Qp07IV6R66kMF/summary
          visualData: 2795.150414098868/521.6459310985977/200/16
        '[Gnh145qZDCCV2B23I5qZQ]:text "Text"':
          data:
            text: |-
              Chunk {{chunk}}/{{total}}:

              {{response}}
          outgoingConnections:
            - output->"Text" wzOfI8FoCi4YPH7SMqzYv/summaries
          visualData: 2095.460140320323/531.2883282385967/300/14
        '[I2obFLvUBKGtYx8arwMeA]:chat "Chat"':
          data:
            cache: true
            frequencyPenalty: 0.5
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0.5
            temperature: 0
            top_p: 1
            useMaxTokensInput: false
            useModelInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          outgoingConnections:
            - response->"Text" Gnh145qZDCCV2B23I5qZQ/response
          visualData: 1841.424761136081/546.4923363640078/200/13
        '[RtTfDTcXP3eigkIWeAhh-]:chunk "Chunk"':
          data:
            model: gpt-4
            numTokensPerChunk: 512
            overlap: 10
            useModelInput: false
          outgoingConnections:
            - chunks->"Text" ZJiOQyn0wI8ifplV9LgQt/data
            - count->"Text" Gnh145qZDCCV2B23I5qZQ/total
            - count->"Text" ZJiOQyn0wI8ifplV9LgQt/total
            - indexes->"Text" Gnh145qZDCCV2B23I5qZQ/chunk
            - indexes->"Text" ZJiOQyn0wI8ifplV9LgQt/chunk
          visualData: 840.3950226023861/508.5530350479365/550.1024107453622/1
        '[ZJiOQyn0wI8ifplV9LgQt]:text "Text"':
          data:
            text: >-
              This is chunk {{chunk}}/{{total}} of the GraphProcessor.ts file.


              Please take notes about this part of the file. Things to mark down are libraries used, techniques used, variable and function names, algorithms, imports, exports especially, etc.


              Be extremely thorough and dense in your response. Do not use superfluous words.

              ```

              {{data}}

              ```
          outgoingConnections:
            - output->"Chat" I2obFLvUBKGtYx8arwMeA/message1
          visualData: 1486.823749346318/511.12600971449524/300/4
        '[_vXnvuuQa4FO01wqqD22k]:chat "Chat"':
          data:
            frequencyPenalty: 0.5
            maxTokens: 1024
            model: gpt-4
            presencePenalty: 0.5
            stop: ""
            temperature: 0.3
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          visualData: 3399.2982395184267/530.0224627304983/200/19
        '[rBQqW1q3Qp07IV6R66kMF]:text "Text"':
          data:
            text: |-
              Here is a summary of a TypeScript file called GraphProcessor.ts:

              """
              {{summary}}
              """

              Can you recreate this file to the best of your ability?
          outgoingConnections:
            - output->"Chat" _vXnvuuQa4FO01wqqD22k/message1
          visualData: 3045.751670546527/529.0821793023815/300/18
        '[wzOfI8FoCi4YPH7SMqzYv]:text "Text"':
          data:
            text: >-
              Please remove any redundancies and superfluous data from this
              summary, including the chunk index, "this section", "this chunk",
              etc.


              {{summaries}}
          outgoingConnections:
            - output->"Chat" F_dKF3vDhO7Cr1c8mvaKP/message1
          visualData: 2440.4366570143184/529.7076073959739/300/15
    MurwaMMNbRHYMOW3runpn:
      metadata:
        description: ""
        id: MurwaMMNbRHYMOW3runpn
        name: Testing
      nodes:
        '[--gHsQqvmvRXzykSV7OA_]:loopController "Loop Controller"':
          data:
            maxIterations: 100
          outgoingConnections:
            - output1->"Assemble Prompt" Hu7Inl7-wPpDh25xPauMz/message1
          visualData: 1050.5768011377613/1853.0830185917564/250/19
        '[7oeiNm0plc2v8AOs6z7yR]:prompt "Prompt"':
          data:
            enableFunctionCall: false
            promptText: Message from assistant
            type: assistant
            useTypeInput: false
          outgoingConnections:
            - output->"Assemble Prompt" Q_wcRYDqOd3HXbJBLljXf/message1
          visualData: 317.4655897358294/1178.5454407886202/250/18
        '[8s2n2hiSw36yezwSOBPQG]:text "Text"':
          data:
            text: Give me a markdown formatted document
          outgoingConnections:
            - output->"Chat" WnxX9EQzXu-cdVJrKH0jj/prompt
          visualData: 783.2821782178219/806.7574257425742/300/null
        '[BFPkIhMdkomg6_MY9keoI]:text "Text"':
          data:
            text: First message
          outgoingConnections:
            - output->"Loop Controller" --gHsQqvmvRXzykSV7OA_/input1Default
          visualData: 646.6520674756624/1884.206508882605/300/28
        '[Hu7Inl7-wPpDh25xPauMz]:assemblePrompt "Assemble Prompt"':
          outgoingConnections:
            - prompt->"Loop Controller" --gHsQqvmvRXzykSV7OA_/input1
          visualData: 1427.358544251912/2090.028394191343/250/27
        '[MS9RaH9CNZTO1ARi-1nMo]:text "Text"':
          data:
            text: Cherry
          outgoingConnections:
            - output->"Array" NiK0-s74J6ciULASTJK4N/input3
          visualData: 335/485/164/3
        '[NiK0-s74J6ciULASTJK4N]:array "Array"':
          data:
            flatten: true
          outgoingConnections:
            - indices->"Text" eQfF99SqPxg3j6VON6kQV/index
            - output->"Text" eQfF99SqPxg3j6VON6kQV/fruit
            - output->"Text" eQfF99SqPxg3j6VON6kQV/input
          visualData: 749/361/200/4
        '[Q_wcRYDqOd3HXbJBLljXf]:assemblePrompt "Assemble Prompt"':
          outgoingConnections:
            - prompt->"Chat" VEidgtZm66j1ZSQe_-1q7/prompt
          visualData: 922.8477668727571/1251.4264019196676/214.7350188075576/15
        '[VEidgtZm66j1ZSQe_-1q7]:chat "Chat"':
          data:
            cache: false
            enableFunctionUse: false
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0
            stop: ""
            temperature: 0.5
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
            useUserInput: false
          visualData: 1380.1170230014266/1186.7739364001902/200/16
        '[WnxX9EQzXu-cdVJrKH0jj]:chat "Chat"':
          data:
            cache: false
            enableFunctionUse: false
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0
            stop: ""
            temperature: 0.5
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
            useUserInput: false
          visualData: 1180.0396039603959/753.6633663366337/200/9
        '[clNSdw87GwbTjiQp7LdQ4]:text "Text"':
          data:
            text: Banana
          outgoingConnections:
            - output->"Array" NiK0-s74J6ciULASTJK4N/input2
          visualData: 334/357/164/2
        '[djvEekj5wmQWV72uGDfIl]:text "Text"':
          data:
            text: This message is appended
          outgoingConnections:
            - output->"Assemble Prompt" Hu7Inl7-wPpDh25xPauMz/message2
          visualData: 1023.1286674573225/2095.465841731702/300/26
        '[eQfF99SqPxg3j6VON6kQV]:text "Text"':
          data:
            text: "[{{index}}] - {{fruit}}"
          isSplitRun: true
          visualData: 1004/359/300/8
        '[iNNukcnXz4AnAUnCPAJD9]:text "Text"':
          data:
            text: Apple
          outgoingConnections:
            - output->"Array" NiK0-s74J6ciULASTJK4N/input1
          visualData: 334/230/164/1
        '[t_zHDgfHtGvTODEKs8mbQ]:prompt "Prompt"':
          data:
            enableFunctionCall: false
            promptText: Message from user
            type: user
            useTypeInput: false
          outgoingConnections:
            - output->"Assemble Prompt" Q_wcRYDqOd3HXbJBLljXf/message2
          visualData: 325.69408534739927/1394.91987872368/250/17
    Pd4dvtJ0K_FosX2rfhTg9:
      metadata:
        description: ""
        id: Pd4dvtJ0K_FosX2rfhTg9
        name: 00 Rivet Analyzer
      nodes:
        '[2NscQMBqyFVwE8LiDZFYg]:text "Text"':
          data:
            text: >-
              Your most relevant memories to "overall analysis" are:


              """

              {{memories}}

              """


              In addition to following up with your last command (by storing memories or thoughts, or by running more commands), you should store memories related to an overall analysis of the Rivet project.


              Only reply with a YAML document. Instead of explaining your analysis, run the REMEMBER_INFO command with your analysis inside the info of the command.
          outgoingConnections:
            - output->"Subgraph" XJjaxKM5N225ejkoyxl_w/prompt_message
          visualData: 4778.988758191404/368.24492671512667/300/75
        '[2UTof8lNm03gvKfFxBDWT]:text "Text"':
          data:
            text: overall analysis
          outgoingConnections:
            - output->"If" NfeMoioEzUx3J_9DSgUWn/value
          visualData: 3883.9730559741065/489.1152144743757/300/73
        '[40B9nTkT_zAkzbA9Rhkpn]:text "Text"':
          data:
            text: /Users/Shared/ironclad/rivet/packages
          outgoingConnections:
            - output->"Prompt" RGaXe9O2ty183OXol7_K2/input
          visualData: 232.485807860262/1022.3493449781658/300/6
        '[7HodDk2GMdHkJC7AzB5CS]:subGraph "Subgraph"':
          data:
            graphId: lLdez5GZqKam_L25xY7CT
          outgoingConnections:
            - output->"Prompt" kBdFzmteK69OqJokqBTb0/memories
          visualData: 3025.9705235840856/527.2202623603948/300/66
        '[7blDJVDN_TbpCvaB-Rkxp]:subGraph "Subgraph"':
          data:
            graphId: 49ADNSJiXKFRvgaRMhFvP
          outgoingConnections:
            - new_messages->"Loop Controller" e7NvtuIgOlA9RMAuUOVt4/input1
          visualData: 3428.0551377526167/738.858001337559/300/69
        '[EN5CwW_fs0KzF8KkWGRja]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: output
          visualData: 1943.673666987144/274.91890798601685/300/18
        '[Iu0BQe4zPuvWNyxWXflPL]:subGraph "Subgraph"':
          data:
            graphId: DBSqt91CfAs1dewM-IhgT
          outgoingConnections:
            - chat_output->"Subgraph" 7HodDk2GMdHkJC7AzB5CS/topic
            - new_messages->"Subgraph" 7blDJVDN_TbpCvaB-Rkxp/current_messages
          visualData: 2515.9237236252075/642.5077894536144/300/89
        '[NfeMoioEzUx3J_9DSgUWn]:if "If"':
          outgoingConnections:
            - output->"Subgraph" iD7KUaQo78VbbWgZbijux/topic
          visualData: 4268.167899208863/484.79841848297406/125/79
        '[QIsP4PLl40qxDwv6hzBYl]:prompt "Prompt"':
          data:
            enableFunctionCall: false
            name: SYSTEM INFORMATION
            promptText: |-
              # Your most relevant memories to "analysis":

              {{memories}}
            type: user
            useTypeInput: false
          outgoingConnections:
            - output->"Assemble Prompt" Xa8H6S7EnNOmNp42PeE6c/message2
          visualData: 578/1204/250/5
        '[RGaXe9O2ty183OXol7_K2]:prompt "Prompt"':
          data:
            enableFunctionCall: false
            name: User
            promptText: "{{input}}"
            type: user
            useTypeInput: false
          outgoingConnections:
            - output->"Assemble Prompt" Xa8H6S7EnNOmNp42PeE6c/message1
          visualData: 590/998/250/2
        '[STjZ9TDdIWSWQETOlEW6j]:subGraph "Subgraph"':
          data:
            graphId: lLdez5GZqKam_L25xY7CT
          outgoingConnections:
            - output->"Prompt" QIsP4PLl40qxDwv6hzBYl/task_list
          visualData: 220.6448947432062/1688.6796416724496/300/84
        '[XJjaxKM5N225ejkoyxl_w]:subGraph "Subgraph"':
          data:
            graphId: 49ADNSJiXKFRvgaRMhFvP
          visualData: 4410.249008329616/764.2026121194765/300/83
        '[Xa8H6S7EnNOmNp42PeE6c]:assemblePrompt "Assemble Prompt"':
          outgoingConnections:
            - prompt->"Loop Controller" e7NvtuIgOlA9RMAuUOVt4/input1Default
          visualData: 1015/833/250/null
        '[Zw6Po0Edgfxls8sgPQ4yn]:text "Text"':
          data:
            text: rivet overall analysis
          outgoingConnections:
            - output->"Subgraph" rYl7JAEelRipYuDpDRHhR/topic
          visualData: -11.348192769476697/1421.2352369244818/160.00087463053285/43
        '[cOtA3LTAJ4OD3BGjXbOxG]:text "Text"':
          data:
            text: task list
          outgoingConnections:
            - output->"Subgraph" STjZ9TDdIWSWQETOlEW6j/topic
          visualData: -14.520185831084863/1698.8668483745391/160.00087463053285/85
        '[e7NvtuIgOlA9RMAuUOVt4]:loopController "Loop Controller"':
          data:
            maxIterations: 100
          outgoingConnections:
            - break->"Graph Output" EN5CwW_fs0KzF8KkWGRja/value
            - output1->"Subgraph" Iu0BQe4zPuvWNyxWXflPL/current_messages
          visualData: 1517.1101924672826/608.9026579155221/250/9
        '[iD7KUaQo78VbbWgZbijux]:subGraph "Subgraph"':
          data:
            graphId: lLdez5GZqKam_L25xY7CT
          outgoingConnections:
            - output->"Text" 2NscQMBqyFVwE8LiDZFYg/memories
          visualData: 4453.391064740932/467.4988106785299/300/77
        '[kBdFzmteK69OqJokqBTb0]:prompt "Prompt"':
          data:
            enableFunctionCall: false
            name: SYSTEM INFORMATION
            promptText: >-
              # Your most relevant memories to your last command:


              {{memories}}


              # Next


              You may either run RECALL_INFO to recall more memories, or you may reflect on the result of your latest command. Then, store as many memories with REMEMBER_INFO as you would like. These can be as detailed or small as you'd like. These memories will be accessible later with RECALL_INFO.
            type: user
            useTypeInput: false
          outgoingConnections:
            - output->"Subgraph" 7blDJVDN_TbpCvaB-Rkxp/prompt_message
          visualData: 3419.141459651317/378.1455376971633/250/65
        '[rYl7JAEelRipYuDpDRHhR]:subGraph "Subgraph"':
          data:
            graphId: lLdez5GZqKam_L25xY7CT
          outgoingConnections:
            - output->"Prompt" QIsP4PLl40qxDwv6hzBYl/memories
          visualData: 217.6087101785059/1373.1105375787274/300/45
    WTeEkN10vbyJnpSBOowvw:
      metadata:
        description: ""
        id: WTeEkN10vbyJnpSBOowvw
        name: Docs
      nodes:
        '[4PTl5m4gQ3PWpP-mezsqp]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: output
          visualData: 1824.638991409224/319.95254020907737/300/13
        '[7mKMzipo-q7OoP3D2i6Ia]:chat "Chat"':
          data:
            cache: false
            enableFunctionUse: false
            frequencyPenalty: 0
            maxTokens: 4096
            model: gpt-4-32k-0613
            presencePenalty: 0
            stop: ""
            temperature: 0.3
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
            useUserInput: false
          outgoingConnections:
            - response->"Prompt" ywivfgGxt7SCyi_-ugZ5X/input
            - response->"User Input" Mnzm_hHdBXR7-tMCuoytx/questions
          visualData: 1829.0649195994358/559.8641943858444/200/7
        '[BzJ0g3_FSPV70p7cnVZNh]:text "Text"':
          data:
            text: >-
              I am working on documentation for my AI IDE called Rivet. It uses
              a visual node editor like in blender and unreal engine, just so
              you know. I am using docusaurus for the docs, and am working on
              documenting every node.


              {{existing_docs_page}}


              Can you make a docs page using the existing one as a template, but for this source file for a node?


              {{source_file}}
          outgoingConnections:
            - output->"Loop Controller" MZ_w9eR32hImFKnkTHtrn/input1Default
          visualData: 812.1989866333732/586.6526333596356/300/null
        '[MZ_w9eR32hImFKnkTHtrn]:loopController "Loop Controller"':
          data:
            maxIterations: 100
          outgoingConnections:
            - break->"Graph Output" 4PTl5m4gQ3PWpP-mezsqp/value
            - output1->"Assemble Prompt" uYrY8xrol_pPkSNVs3hNV/message1
            - output1->"Chat" 7mKMzipo-q7OoP3D2i6Ia/prompt
          visualData: 1192.12520774107/609.5158402552806/250/null
        '[Mnzm_hHdBXR7-tMCuoytx]:userInput "User Input"':
          data:
            prompt: This is an example question?
            useInput: true
          outgoingConnections:
            - output->"Assemble Prompt" uYrY8xrol_pPkSNVs3hNV/message3
          visualData: 2377.909126290767/767.7690984682896/250/12
        '[VtwWldPN8RBa7V9rSNQzt]:text "Text"':
          data:
            text: >-
              Here is the existing docs page for the `JoinNode`:


              ```

              ---

              id: join

              title: Join Node

              sidebar_label: Join

              ---


              import Tabs from '@theme/Tabs';

              import TabItem from '@theme/TabItem';

              import Admonition from '@theme/Admonition';


              ## Overview


              The Join Node is used to concatenate multiple strings using a specified delimiter. The delimiter can be set directly in the node or provided as an input.


              ![Join Node Screenshot](./assets/join-node.png)


              <Tabs
                defaultValue="inputs"
                values={[
                  {label: 'Inputs', value: 'inputs'},
                  {label: 'Outputs', value: 'outputs'},
                  {label: 'Editor Settings', value: 'settings'},
                ]
              }>


              <TabItem value="inputs">


              ## Inputs


              | Title                  | Data Type | Description                                                                  | Default Value | Notes                                                            |

              | ---------------------- | --------- | ---------------------------------------------------------------------------- | ------------- | ---------------------------------------------------------------- |

              | Join String (optional) | string    | The string to be used as a delimiter when `Use Join String Input` is enabled | `\n`          | Only appears if the `Use Join String Input` setting is enabled   |

              | Input [i]              | string    | The ith string input to be joined                                            | N/A           | Dynamic number of inputs based on how many connections there are |


              </TabItem>


              <TabItem value="outputs">


              ## Outputs


              | Title  | Data Type | Description             | Notes                                                                             |

              | ------ | --------- | ----------------------- | --------------------------------------------------------------------------------- |

              | Joined | string    | The concatenated string | The output will be a single string resulting from joining all valid string inputs |


              </TabItem>


              <TabItem value="settings">


              ## Editor Settings


              | Setting     | Description                                               | Default Value | Use Input Toggle |

              | ----------- | --------------------------------------------------------- | ------------- | ---------------- |

              | Flatten     | If enabled, array inputs will be flattened before joining | true          | No               |

              | Join String | The string to be used as a delimiter when joining inputs  | `\n`          | Yes              |


              </TabItem>


              </Tabs>


              ### Example 1: Joining Two Text Nodes


              Let's say you have two Text nodes: one with the value `"Hello"` and the other with the value `"World"`. If you want to concatenate these two strings with a newline in between, you can do the following:


              1. Add a Join Node to your graph.

              2. Connect the output of the first Text node to `Input 1` of the Join Node.

              3. Connect the output of the second Text node to `Input 2` of the Join Node.

              4. Leave the `Join String` setting as the default value (`\n`).


              The `Joined` output of the Join Node will now be `"Hello\nWorld"`.


              ### Example 2: Joining an Array with a Custom Delimiter


              Imagine you have an Array node with the value `["One", "Two", "Three"]` and you want to join these values with a `/` delimiter. Here's how to do it:


              1. Add a Join Node to your graph.

              2. Connect the output of the Array node to `Input 1` of the Join Node.

              3. Change the `Join String` setting to `"/"`.


              The `Joined` output of the Join Node will now be `"One/Two/Three"`.


              ### Example 3: Connecting a Split Node to a Join Node


              Suppose you have a node with `Split` enabled, which runs N times in parallel and outputs an array of results. If you want to join these results into a single string, you can do the following:


              1. Add a Join Node to your graph.

              2. Connect the output of the node with `Split` enabled to `Input 1` of the Join Node.

              3. Set the `Join String` setting to the delimiter you want to use.


              The `Joined` output of the Join Node will now be a string that joins all the results from the Split node. For instance, if the Split node outputs `["Hello", "World"]` and the `Join String` is `" "`, the `Joined` output will be `"Hello World"`.


              1. **Dynamic Join String:** The Join Node allows for dynamic join strings. If you have a node that generates a delimiter string dynamically, you can use this as your join string by enabling the `Use Join String Input` setting and connecting the output of that node to the `Join String` input of the Join Node.


              2. **Multiple Inputs:** The Join Node can handle any number of inputs. If you have several nodes generating strings and you want to join all of them, you can simply connect all of them to the Join Node. The Join Node will automatically adjust the number of `Input [i]` inputs based on the number of connections.


              3. **Handling Missing Inputs:** If an input is missing or not a string, the Join Node will simply ignore it during the joining process. This can be useful when dealing with optional inputs that may not always be present.


              4. **Special Characters as Delimiters:** You can use special characters as delimiters. This includes newline (`\n`), tab (`\t`), and space (` `).


              ## Flattening Arrays


              The Join Node has a `Flatten` setting. When enabled, this setting treats each element of an array input as a separate string to be joined. This is particularly useful when you're dealing with array inputs and want to concatenate all elements of an array into a single string.


              Flattening in the Join Node works similarly to the [Array Node](./array). For more information on how flattening works, refer to the documentation for the Array Node.


              ## Error Handling


              If `Use Join String Input` is enabled but no valid `Join String` input is provided, the Join Node will default to the `Join String` specified in the node data. If no valid string inputs are provided, the output will be an empty string.


              **Q: What happens if I connect a non-string node to the Join Node?**


              A: The Join Node is designed to work with string inputs. If a non-string input is connected, the node will attempt to convert it to a string. If this conversion isn't possible, the input will be ignored during the joining process.


              **Q: Can I connect an Array Node to the Join Node?**


              A: Yes, you can connect an Array Node to the Join Node. If the `Flatten` setting is enabled, each element of the array will be treated as a separate string to be joined. If `Flatten` is disabled, the array will be converted to a string representation and then joined.


              **Q: What happens if I use a dynamic join string but the input is missing or not a string?**


              A: If the `Use Join String Input` setting is enabled but no valid `Join String` input is provided, the Join Node will default to the `Join String` specified in the node data.


              **Q: How does the Join Node handle empty strings or null values?**


              A: The Join Node will ignore any inputs that are null or not a string. Empty strings, however, will be included in the join operation. For example, if your inputs are `"Hello"`, `""`, and `"World"`, and your join string is `" "`, the output will be `"Hello  World"` (note the extra space between "Hello" and "World").


              ## See Also

              ```
          outgoingConnections:
            - output->"Text" BzJ0g3_FSPV70p7cnVZNh/existing_docs_page
          visualData: 337.15066883147637/416.09957189316515/300/1
        '[_fpeIdnKWH_osAtkxQmyz]:userInput "User Input"':
          data:
            prompt: Paste in the file to write docs for
            useInput: false
          outgoingConnections:
            - output->"Text" BzJ0g3_FSPV70p7cnVZNh/source_file
          visualData: 378.2262752579562/763.4563305866574/250/2
        '[iD3sWLJf7G-CHBZpsaB9L]:text "Text"':
          data:
            text: You are a documentation assistant that can help write documentation for an
              application when given examples and source code.
          outgoingConnections:
            - output->"Chat" 7mKMzipo-q7OoP3D2i6Ia/systemPrompt
          visualData: 1183.9419997089549/356.3468775151959/300/5
        '[uYrY8xrol_pPkSNVs3hNV]:assemblePrompt "Assemble Prompt"':
          outgoingConnections:
            - prompt->"Loop Controller" MZ_w9eR32hImFKnkTHtrn/input1
          visualData: 2861.2599254739066/748.9590017775972/250/11
        '[ywivfgGxt7SCyi_-ugZ5X]:prompt "Prompt"':
          data:
            enableFunctionCall: false
            promptText: "{{input}}"
            type: assistant
            useTypeInput: false
          outgoingConnections:
            - output->"Assemble Prompt" uYrY8xrol_pPkSNVs3hNV/message2
          visualData: 2106.8544425110695/601.0461251185214/250/8
    Z58ZSUt6PzrLJYSKKdmMk:
      metadata:
        description: ""
        id: Z58ZSUt6PzrLJYSKKdmMk
        name: Docs 2
      nodes:
        '[2hFef_YClkWthsXSdfIpz]:prompt "Prompt"':
          data:
            enableFunctionCall: false
            promptText: "{{input}}"
            type: assistant
            useTypeInput: false
          outgoingConnections:
            - output->"Assemble Prompt" C4eG4iCgiuErt-jkU5FNW/message2
          visualData: 2106.8544425110695/601.0461251185214/250/8
        '[C4eG4iCgiuErt-jkU5FNW]:assemblePrompt "Assemble Prompt"':
          outgoingConnections:
            - prompt->"Loop Controller" GBKitGL6d89jMwz4KD--U/input1
          visualData: 2861.2599254739066/748.9590017775972/250/11
        '[GBKitGL6d89jMwz4KD--U]:loopController "Loop Controller"':
          data:
            maxIterations: 100
          outgoingConnections:
            - break->"Graph Output" jXLfU1WHl-2pRmVlluCNp/value
            - output1->"Assemble Prompt" C4eG4iCgiuErt-jkU5FNW/message1
            - output1->"Chat" UWBFvj9LZsaT5Fmczqy6B/prompt
          visualData: 1192.12520774107/609.5158402552806/250/null
        '[IK_7V7t62MtOGLVThoAbs]:userInput "User Input"':
          data:
            prompt: This is an example question?
            useInput: true
          outgoingConnections:
            - output->"Assemble Prompt" C4eG4iCgiuErt-jkU5FNW/message3
          visualData: 2377.909126290767/767.7690984682896/250/12
        '[IaDN_IvRyCyd4w5OSbU1m]:text "Text"':
          data:
            text: >-
              I am working on documentation for my AI IDE called Rivet. It uses
              a visual node editor like in blender and unreal engine, just so
              you know. I am using docusaurus for the docs, and am working on
              documenting every node.


              Here is an existing docs page:


              ```

              {{page}}

              ```


              {{question}}
          outgoingConnections:
            - output->"Loop Controller" GBKitGL6d89jMwz4KD--U/input1Default
          visualData: 812.1989866333732/586.6526333596356/300/null
        '[MtVQBLQTrG8OIkGYEZE40]:userInput "User Input"':
          data:
            prompt: What question do you have?
            useInput: false
          outgoingConnections:
            - output->"Text" IaDN_IvRyCyd4w5OSbU1m/question
          visualData: 422.5936698889692/768.7259474555756/250/15
        '[UWBFvj9LZsaT5Fmczqy6B]:chat "Chat"':
          data:
            cache: false
            enableFunctionUse: false
            frequencyPenalty: 0
            maxTokens: 4096
            model: gpt-4-32k-0613
            presencePenalty: 0
            stop: ""
            temperature: 0.3
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
            useUserInput: false
          outgoingConnections:
            - response->"Prompt" 2hFef_YClkWthsXSdfIpz/input
            - response->"User Input" IK_7V7t62MtOGLVThoAbs/questions
          visualData: 1829.0649195994358/559.8641943858444/200/7
        '[WHWxlLVSal4QpXV_NYmi0]:userInput "User Input"':
          data:
            prompt: Paste in the existing docs page
            useInput: false
          outgoingConnections:
            - output->"Text" IaDN_IvRyCyd4w5OSbU1m/page
            - output->"Text" IaDN_IvRyCyd4w5OSbU1m/source_file
          visualData: 422.5936698889692/558.4070202649488/250/14
        '[jXLfU1WHl-2pRmVlluCNp]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: output
          visualData: 1824.638991409224/319.95254020907737/300/13
        '[qNHEy4-4-T4taIBwubuAU]:text "Text"':
          data:
            text: You are a documentation assistant that can help write documentation for an
              application when given examples and source code.
          outgoingConnections:
            - output->"Chat" UWBFvj9LZsaT5Fmczqy6B/systemPrompt
          visualData: 1183.9419997089549/356.3468775151959/300/5
    cWbUqJZE2osNm42Q2ewC9:
      metadata:
        description: ""
        id: cWbUqJZE2osNm42Q2ewC9
        name: RA - System Commands
      nodes:
        '[WI9_Qj2uuV6QyYUB6Gel7]:graphOutput "Graph Output"':
          data:
            dataType: chat-message
            id: output
          visualData: 1112/503/300/null
        '[vN2ea8R1hCHDK7sIBUgW7]:prompt "Prompt"':
          data:
            enableFunctionCall: false
            name: SYSTEM INFORMATION
            promptText: >-
              # System Commands


              You have access to System Commands to perform actions such as iteracting with the file system and other APIs.


              System commands are called by using a YAML document like this:


              ```yaml

              systemCommands:
                - name: COMMAND_NAME
                  arguments:
                    some-argument: some argument value
                - name: OTHER_COMMAND_NAME
                  arguments:
                    some-other-argument: some other argument value
              ```


              When multiple commands are specified, all of the commands will be ran in parallel. You can run at most 5 commands at once in parallel.


              ## System Command List


              ### LIST_FILES_IN_DIRECTORY


              Lists all files recursively in the Rivet directory.


              Arguments:
                directory: The directory to read.


              ### ASK_QUESTION_ABOUT_FILE


              Reads a file and asks another AI to answer a question about the file. You can use this to do things like summarize a file, extract key points about the file, etc. This is a very powerful command to use!


              Arguments:
                path: the path to the file to read
                question: |
                  The question to ask about the file. Be extremely detailed about what your questions are and what you are looking for in regards to this file. Give the AI as much information as you can.

              ### REMEMBER_INFO


              This function is important - it will store a block of text in your long-term memory. Call this function with a block of text, and when something relevant comes up, it will be included in the response.


              Arguments:
                id: [optional] - Unique identifier for the memory. If you save a memory with the same ID as an existing one, it will overwrite the existing one.
                info: |
                  The information to remember. Be extremely detailed as possible here. Include things such as file name, reasons for remembering this info, etc.


              ### RECALL_INFO


              Use this function to search for any previously stored memories related to a topic.


              Arguments:
                topic: The topic to search about. Be extremely detailed about what your topic is, use as many words as possible to narrowthings down.
            type: user
            useTypeInput: false
          outgoingConnections:
            - output->"Graph Output" WI9_Qj2uuV6QyYUB6Gel7/value
          visualData: 612/503/250/1
    d6Pgmz7n8qvXkaNF-2e9P:
      metadata:
        description: ""
        id: d6Pgmz7n8qvXkaNF-2e9P
        name: List Rivet Files
      nodes:
        '[FSHys4NrHcjHQvSE_Rv7-]:graphOutput "Graph Output"':
          data:
            dataType: string[]
            id: files
          visualData: -553/644/300/24
        '[oz0gemjC0VdsTeemnG_Kp]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: root_path
          visualData: -556/443/300/25
        '[wNG3QGg-NzvkTyoaynBz_]:readDirectory "Read Directory"':
          data:
            filterGlobs:
              - "**/*.{ts,tsx,css}"
            ignores:
              - "**/dist/**"
              - "**/src-tauri/**"
            includeDirectories: false
            path: /Users/shared/ironclad/rivet/packages
            recursive: true
            relative: true
            useFilterGlobsInput: false
            useIncludeDirectoriesInput: false
            usePathInput: false
            useRecursiveInput: false
            useRelativeInput: false
          outgoingConnections:
            - paths->"Graph Output" FSHys4NrHcjHQvSE_Rv7-/value
            - rootPath->"Graph Output" oz0gemjC0VdsTeemnG_Kp/value
          visualData: -1109.8933869703742/414.52061856748116/null/23
    g_lk18P0-BSirYNE2qGAa:
      metadata:
        description: ""
        id: g_lk18P0-BSirYNE2qGAa
        name: "RA - Command: RECALL_INFO"
      nodes:
        '[1rr5wAWLGDR8OD_DtZBsj]:text "Text"':
          data:
            text: "3"
          outgoingConnections:
            - output->"If/Else" 5NK8cYmHbe2aK2FNiCOET/false
          visualData: 644.186134866143/828.639015006256/143.43331570736757/20
        '[5NK8cYmHbe2aK2FNiCOET]:ifElse "If/Else"':
          outgoingConnections:
            - output->"Text" wNNsGDoazltbd0zriLypI/n
            - output->"Vector KNN" UodDoY4FFQD-ghaAND19v/k
          visualData: 971.7547296627853/774.2293162095256/125/null
        '[5Va0TC0xtEPJHrqjqzTNW]:graphInput "Graph Input"':
          data:
            dataType: object
            id: arguments
            useDefaultValueInput: true
          outgoingConnections:
            - data->"Extract Object Path" s9s3oX5pAKvtV9JuOYfFt/object
            - data->"Extract Object Path" yc49PU2tC9-LPQmPPi28M/object
          visualData: 206.18399339933993/447.2013201320132/300/18
        '[8vwMe_uU1mH6sFC3EbuO0]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: output
          visualData: 3054.0182190152213/325.4223064295404/300/40
        '[HDxFalAxE-LpUV_-KRVc1]:getEmbedding "Get Embedding"':
          data:
            integration: openai
            useIntegrationInput: false
          outgoingConnections:
            - embedding->"Vector KNN" UodDoY4FFQD-ghaAND19v/vector
          visualData: 1033.9087435447254/383.59221170727943/200/23
        '[SZt7GbwaokaGx5a0bzXoW]:extractObjectPath "Extract Object Path"':
          data:
            path: $.id
            usePathInput: false
          isSplitRun: true
          outgoingConnections:
            - match->"Text" zef9WfSbXrV_24nfiBXQj/id
          visualData: 1787.719200663156/389.5547467012547/250/36
        '[UodDoY4FFQD-ghaAND19v]:vectorNearestNeighbors "Vector KNN"':
          data:
            collectionId: rivet-3ff65ea.svc.us-west1-gcp-free.pinecone.io/test1
            integration: pinecone
            k: 10
            useKInput: true
          outgoingConnections:
            - results->"Extract Object Path" ng2J56HcO9blylcpLLg_2/object
          visualData: 1279.3360881667509/408.90705286005004/200/24
        '[i3--YrY0fSi5WBL7uhrJG]:extractJson "Extract JSON"':
          outgoingConnections:
            - output->"Graph Input" 5Va0TC0xtEPJHrqjqzTNW/default
          visualData: -23.875216125178355/481.0266450929813/132.0370817950229/27
        '[ng2J56HcO9blylcpLLg_2]:extractObjectPath "Extract Object Path"':
          data:
            path: $[*]
            usePathInput: false
          outgoingConnections:
            - all_matches->"Extract Object Path" SZt7GbwaokaGx5a0bzXoW/object
            - all_matches->"Extract Object Path" tibz7T4GvEV5UFblNUsFN/object
          visualData: 1515.6296364360278/485.50903746734343/250/38
        '[s9s3oX5pAKvtV9JuOYfFt]:extractObjectPath "Extract Object Path"':
          data:
            path: $.topic
            usePathInput: false
          outgoingConnections:
            - match->"Get Embedding" HDxFalAxE-LpUV_-KRVc1/input
            - match->"Text" wNNsGDoazltbd0zriLypI/topic
          visualData: 683/375/250/2
        '[tibz7T4GvEV5UFblNUsFN]:extractObjectPath "Extract Object Path"':
          data:
            path: $.data
            usePathInput: false
          isSplitRun: true
          outgoingConnections:
            - match->"Text" zef9WfSbXrV_24nfiBXQj/data
          visualData: 1790.348085341679/577.6223834317746/250/37
        '[wNNsGDoazltbd0zriLypI]:text "Text"':
          data:
            text: |-
              You have asked for memories related to the topic "{{topic}}".

              The closest {{n}} memories you have related to this topic are:

              {{memories}}
          outgoingConnections:
            - output->"Graph Output" 8vwMe_uU1mH6sFC3EbuO0/value
          visualData: 2700.733848787849/275.1040318143705/300/41
        '[yc49PU2tC9-LPQmPPi28M]:extractObjectPath "Extract Object Path"':
          data:
            path: $.count
            usePathInput: false
          outgoingConnections:
            - match->"If/Else" 5NK8cYmHbe2aK2FNiCOET/if
            - match->"If/Else" 5NK8cYmHbe2aK2FNiCOET/true
          visualData: 674.3407590759076/582.3745874587459/250/17
        '[zEyCZMZyTvWfLGWRsHles]:text "Text"':
          data:
            text: |-
              {
                  "topic": "This is a test topic"
              }
          outgoingConnections:
            - output->"Extract JSON" i3--YrY0fSi5WBL7uhrJG/input
          visualData: -439.48875352178374/498.85824900768716/300/28
        '[zef9WfSbXrV_24nfiBXQj]:text "Text"':
          data:
            text: "{{id}}: {{data}}"
          isSplitRun: true
          outgoingConnections:
            - output->"Text" wNNsGDoazltbd0zriLypI/memories
          visualData: 2214.912960923141/436.8746709146683/300/42
    lLdez5GZqKam_L25xY7CT:
      metadata:
        description: ""
        id: lLdez5GZqKam_L25xY7CT
        name: RA - Most Relevant Memories
      nodes:
        '[GZvTLJWLzabc_STi3EBnj]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: output
          visualData: 3341.0988051363474/1511.1227266506091/300/null
        '[LCJn4Mvw7nBHLm2AG6055]:extractObjectPath "Extract Object Path"':
          data:
            path: $.data
            usePathInput: false
          isSplitRun: true
          outgoingConnections:
            - match->"Text" Qvo5o6T6k92xZssPwg3Kj/content
          visualData: 2155.411694615173/1724.7461192834876/250/47
        '[LjiWBOjBx2tMH7uaQXvP0]:subGraph "Subgraph"':
          data:
            graphId: 1C7HNzd5v7ULqf04-5Uob
          outgoingConnections:
            - items->"Extract Object Path" VQ3q88Yuk9bejSBolE3tK/object
          visualData: 1353.1101608588772/1511.1227266506091/300/49
        '[Qvo5o6T6k92xZssPwg3Kj]:text "Text"':
          data:
            text: "{{id}}: {{content}}"
          isSplitRun: true
          outgoingConnections:
            - output->"Text" xTnmDbxMqN6p3LMJsqaUg/memories
          visualData: 2475.1347789044294/1615.4979308949553/300/48
        '[VQ3q88Yuk9bejSBolE3tK]:extractObjectPath "Extract Object Path"':
          data:
            path: $[*]
            usePathInput: false
          outgoingConnections:
            - all_matches->"Extract Object Path" LCJn4Mvw7nBHLm2AG6055/object
            - all_matches->"Extract Object Path" pwpe07-IOJqChP5fETDzu/object
          visualData: 1765.6666125270294/1558.2787282899076/250/50
        '[WZXnEWHrAnUtfoOw_XEdz]:graphInput "Graph Input"':
          data:
            dataType: string
            id: topic
          outgoingConnections:
            - data->"Subgraph" LjiWBOjBx2tMH7uaQXvP0/topic
          visualData: 853.1101608588772/1511.1227266506091/300/null
        '[pwpe07-IOJqChP5fETDzu]:extractObjectPath "Extract Object Path"':
          data:
            path: $.id
            usePathInput: false
          isSplitRun: true
          outgoingConnections:
            - match->"Text" Qvo5o6T6k92xZssPwg3Kj/id
          visualData: 2155.411694615173/1513.767858621101/250/null
        '[xTnmDbxMqN6p3LMJsqaUg]:text "Text"':
          data:
            text: "{{memories}}"
          outgoingConnections:
            - output->"Graph Output" GZvTLJWLzabc_STi3EBnj/value
          visualData: 2841.0988051363474/1590.3957052689395/300/51
    lXUC_1uq9nRw3WNXCkpdB:
      metadata:
        description: ""
        id: lXUC_1uq9nRw3WNXCkpdB
        name: "RA - Command: READ_DIRECTORY"
      nodes:
        '[2sQqhGNeRlvjvM-LodS8R]:extractObjectPath "Extract Object Path"':
          data:
            path: $.directory
            usePathInput: false
          outgoingConnections:
            - match->"Read Directory" YDnARl29utdEx5tX_nt-a/path
          visualData: 250.01267630499774/366.5850974833843/250/15
        '[TP1i6OVQF_SnmAuhhc3yf]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: output
          visualData: 1660.1381498617586/402.9919627292449/300/11
        '[U6X1HmD-JnYkzrw0KA3ad]:text "Text"':
          data:
            text: "{{input}}"
          outgoingConnections:
            - output->"Graph Output" TP1i6OVQF_SnmAuhhc3yf/value
          visualData: 1466.3956685691494/425.3468644168537/150.96732208260823/8
        '[YDnARl29utdEx5tX_nt-a]:readDirectory "Read Directory"':
          data:
            filterGlobs: []
            ignores:
              - "**/node_modules/**"
              - "**/src-tauri/**"
              - "**/dist/**"
              - "**/app-executor/bin/**"
              - "**/*.wasm"
            includeDirectories: false
            path: /Users/Shared/ironclad/rivet/packages
            recursive: true
            relative: false
            useFilterGlobsInput: false
            useIgnoresInput: false
            useIncludeDirectoriesInput: false
            usePathInput: true
            useRecursiveInput: false
            useRelativeInput: false
          outgoingConnections:
            - paths->"Text" y-sDq1vd52ctHX0NCTWQb/path
          visualData: 651.5571857432192/348.8015299069658/null/19
        '[_78OsjTMGMAugz-VIcct4]:graphInput "Graph Input"':
          data:
            dataType: object
            id: arguments
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Extract Object Path" 2sQqhGNeRlvjvM-LodS8R/object
          visualData: -125.57242117838655/357/300/14
        '[y-sDq1vd52ctHX0NCTWQb]:text "Text"':
          data:
            text: "{{path}}"
          isSplitRun: true
          outgoingConnections:
            - output->"Text" U6X1HmD-JnYkzrw0KA3ad/input
          splitRunMax: 1000
          visualData: 1253.3705377832819/424.3104028500138/177.04143789015916/7
    pgbl6cyJ6kZWZwEb0o7rz:
      metadata:
        description: ""
        id: pgbl6cyJ6kZWZwEb0o7rz
        name: "RA - Command: REMEMBER_INFO"
      nodes:
        '[3ULFu_KZhCNX9S4bTKSbI]:text "Text"':
          data:
            text: The information has been remembered.
          outgoingConnections:
            - output->"If" UV0GD007NdcPBWTThvUXn/value
          visualData: 1343.433220863393/676.8483689406404/300/20
        '[EkSk0XXiGO9ugWxWlPhB4]:extractObjectPath "Extract Object Path"':
          data:
            path: $.info
            usePathInput: false
          outgoingConnections:
            - match->"Get Embedding" tu5GLMWeIMTT_IJLUVVlq/input
            - match->"Hash" xLo2UmlnOqD3OYXby4bSZ/input
            - match->"Vector Store" MeZhLHLCVWDKzWtlOQeQb/data
          visualData: 683/375/250/2
        '[MeZhLHLCVWDKzWtlOQeQb]:vectorStore "Vector Store"':
          data:
            collectionId: rivet-3ff65ea.svc.us-west1-gcp-free.pinecone.io/test1
            integration: pinecone
          outgoingConnections:
            - complete->"If" UV0GD007NdcPBWTThvUXn/if
          visualData: 1408/389/200/8
        '[RbHUSAcTUJTsK2sRlzbJQ]:ifElse "If/Else"':
          outgoingConnections:
            - output->"Text" 3ULFu_KZhCNX9S4bTKSbI/id
            - output->"Vector Store" MeZhLHLCVWDKzWtlOQeQb/id
          visualData: 1046.2140580979324/662.0972381436029/125/18
        '[UV0GD007NdcPBWTThvUXn]:if "If"':
          outgoingConnections:
            - output->"Graph Output" YoSsNCM2sMjGi5UwLhP9Z/value
          visualData: 1722/436/125/12
        '[YoSsNCM2sMjGi5UwLhP9Z]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: output
          visualData: 2423.453795379538/216.9381188118812/300/21
        '[dL7Bop8CO27-xbZKDoKF0]:extractObjectPath "Extract Object Path"':
          data:
            path: $.id
            usePathInput: false
          outgoingConnections:
            - match->"If/Else" RbHUSAcTUJTsK2sRlzbJQ/if
            - match->"If/Else" RbHUSAcTUJTsK2sRlzbJQ/true
          visualData: 683/568.519801980198/250/16
        '[tu5GLMWeIMTT_IJLUVVlq]:getEmbedding "Get Embedding"':
          data:
            integration: openai
            useIntegrationInput: false
          outgoingConnections:
            - embedding->"Vector Store" MeZhLHLCVWDKzWtlOQeQb/vector
          visualData: 1065/267/200/7
        '[v0cI-5wGxCzMVQ2MhEl_z]:graphInput "Graph Input"':
          data:
            dataType: object
            id: arguments
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Extract Object Path" EkSk0XXiGO9ugWxWlPhB4/object
            - data->"Extract Object Path" dL7Bop8CO27-xbZKDoKF0/object
          visualData: 343/371/300/1
        '[xLo2UmlnOqD3OYXby4bSZ]:hash "Hash"':
          data:
            algorithm: sha256
          outgoingConnections:
            - hash->"If/Else" RbHUSAcTUJTsK2sRlzbJQ/false
          visualData: 655.8735656404247/821.8014999261702/250/19
    sgqPsMPVjV-hHvsAloa3y:
      metadata:
        description: ""
        id: sgqPsMPVjV-hHvsAloa3y
        name: "RA - Command: READ_FILE"
      nodes:
        '[1zWd9IKU6oivnbRfSOulB]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: output
          visualData: 1655.4237156131949/344.06153462220004/300/16
        '[3zvt1Wdg0WAtkS_gy4Le2]:text "Text"':
          data:
            text: |-
              ```
              // {{file_name}}

              {{contents}}
              ```
          outgoingConnections:
            - output->"Graph Output" 1zWd9IKU6oivnbRfSOulB/value
          visualData: 1310.636375619824/301.4545670474145/300/15
        '[O4TUctRQ9cTMpuBfy07TM]:graphInput "Graph Input"':
          data:
            dataType: object
            id: arguments
            useDefaultValueInput: false
          outgoingConnections:
            - data->"Extract Object Path" gCLcAGjsk4qroZ7F0_3I0/object
          visualData: 239/357/300/1
        '[gCLcAGjsk4qroZ7F0_3I0]:extractObjectPath "Extract Object Path"':
          data:
            path: $.path
            usePathInput: false
          outgoingConnections:
            - match->"Read File" t_g4bWwfU5_b9X6pzk6OS/path
            - match->"Text" 3zvt1Wdg0WAtkS_gy4Le2/file_name
          visualData: 613/365/250/2
        '[t_g4bWwfU5_b9X6pzk6OS]:readFile "Read File"':
          data:
            errorOnMissingFile: false
            path: ""
            usePathInput: true
          outgoingConnections:
            - content->"Text" 3zvt1Wdg0WAtkS_gy4Le2/contents
          visualData: 907.7722794413804/405.417269725349/250/12
    u6yVHvgJi01zZYY_5f4y3:
      metadata:
        description: ""
        id: u6yVHvgJi01zZYY_5f4y3
        name: RA - Exec Commands
      nodes:
        '[6aKpPSGTWQarC7SnxrtH5]:subGraph "Subgraph"':
          data:
            graphId: 9cRiigw77WY0G_wWDYp4Z
          isSplitRun: true
          outgoingConnections:
            - command_output->"Text" HG6cDo_qEFKjH_FiOyBQC/command
          visualData: 710.6904790341357/376.5930426926441/300/2
        '[HG6cDo_qEFKjH_FiOyBQC]:text "Text"':
          data:
            text: |-
              ---

              {{command}}
          isSplitRun: true
          outgoingConnections:
            - output->"Prompt" l2Lne7i6GCD4UBUCBmwEI/input
          visualData: 1128.3888320265673/370.2191258971111/300/4
        '[TQlCAaArLhkbt7Z-YE_Ol]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: commands_output
          visualData: 2162.433555324635/376.65765337302344/300/13
        '[a_EfAnKA_WZ3QprwFJ_Qm]:if "If"':
          outgoingConnections:
            - output->"Subgraph" 6aKpPSGTWQarC7SnxrtH5/command
          visualData: 470.56038435942/481.8114644600358/125/16
        '[gqKd_dWEPpEZHfcUNWGYW]:ifElse "If/Else"':
          outgoingConnections:
            - output->"Graph Output" TQlCAaArLhkbt7Z-YE_Ol/value
          visualData: 1900.9010140792252/421.6721425286348/125/15
        '[l2Lne7i6GCD4UBUCBmwEI]:prompt "Prompt"':
          data:
            enableFunctionCall: false
            name: SYSTEM COMMAND
            promptText: "{{input}}"
            type: user
            useTypeInput: false
          outgoingConnections:
            - output->"If/Else" gqKd_dWEPpEZHfcUNWGYW/true
          visualData: 1481.653703243058/377.13686377224553/250/7
        '[pthEZe9kZ5WqR5qI4-Vq6]:prompt "Prompt"':
          data:
            enableFunctionCall: false
            name: SYSTEM INFORMATION
            promptText: No commands found to run!
            type: user
            useTypeInput: false
          outgoingConnections:
            - output->"If/Else" gqKd_dWEPpEZHfcUNWGYW/false
          visualData: 1466.9226639256026/654.1024948581031/250/17
        '[wOGBtU6aVMyEK5DuRnTXe]:graphInput "Graph Input"':
          data:
            dataType: object[]
            id: commands
            useDefaultValueInput: false
          outgoingConnections:
            - data->"If" a_EfAnKA_WZ3QprwFJ_Qm/if
            - data->"If" a_EfAnKA_WZ3QprwFJ_Qm/value
            - data->"If/Else" gqKd_dWEPpEZHfcUNWGYW/if
          visualData: -6.4520287661832185/346.62538707922704/300/9
    xbw1yaJUIdwWrJH3WU4fR:
      metadata:
        description: ""
        id: xbw1yaJUIdwWrJH3WU4fR
        name: RA - Chat Always System Command
      nodes:
        '[2_KA9pGsv83JjkPd1MV4q]:loopController "Loop Controller"':
          data:
            maxIterations: 100
          outgoingConnections:
            - break->"Extract Object Path" JLLQ3v_VxM6IXG32tlhN7/object
            - output1->"Assemble Prompt" 92_icEvqGJNAYqfRkKrnr/message1
            - output1->"Chat" 92yohvA5jf_4rF0wdgvmp/prompt
          visualData: 1533.8593700109845/649.284081111641/250/77
        '[92_icEvqGJNAYqfRkKrnr]:assemblePrompt "Assemble Prompt"':
          outgoingConnections:
            - prompt->"Loop Controller" 2_KA9pGsv83JjkPd1MV4q/input1
          visualData: 3183.8551445458884/1025.5150016819186/250/97
        '[92yohvA5jf_4rF0wdgvmp]:chat "Chat"':
          data:
            cache: false
            enableFunctionUse: false
            frequencyPenalty: 0
            maxTokens: 4096
            model: gpt-4-0613
            presencePenalty: 0
            stop: ""
            temperature: 0.5
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
            useUserInput: false
          outgoingConnections:
            - response->"Loop Controller" 2_KA9pGsv83JjkPd1MV4q/input2
            - response->"Subgraph" KurJE2VRa6wnuAIMZ0yyP/input
          visualData: 1826.1340912307267/588.4484887071343/200/79
        '[ChlRAn2eL4ndq2VSH0sUb]:text "Text"':
          data:
            text: "true"
          outgoingConnections:
            - output->"If/Else" m53Fm7rEKyAQRqIg7RL0Y/false
          visualData: 2295.886404047394/925.0804762754461/300/94
        '[CzhSF51XkMyp1dbJ5h7VO]:graphInput "Graph Input"':
          data:
            dataType: string
            id: systemPrompt
          outgoingConnections:
            - data->"Chat" 92yohvA5jf_4rF0wdgvmp/systemPrompt
          visualData: 1075.8448771225335/539.0283254118859/300/73
        '[JLLQ3v_VxM6IXG32tlhN7]:extractObjectPath "Extract Object Path"':
          data:
            path: $[1]
            usePathInput: false
          outgoingConnections:
            - match->"Graph Output" wkQn6ctmCI1yXdo6sGGBz/value
          visualData: 1888.7070328821937/336.1543563829915/250/null
        '[KurJE2VRa6wnuAIMZ0yyP]:subGraph "Subgraph"':
          data:
            graphId: 3BWUibK-Zr_i2GjKA277_
          outgoingConnections:
            - commands->"If/Else" f5fMuI0pWIHCyvx9aU4ZU/if
            - commands->"If/Else" m53Fm7rEKyAQRqIg7RL0Y/if
          visualData: 2095.0173532344493/633.3420929518835/300/80
        '[SoW97Q838zltZj6QFkfeb]:prompt "Prompt"':
          data:
            enableFunctionCall: false
            name: SYSTEM INFORMATION
            promptText: "{{input}}"
            type: user
            useTypeInput: false
          outgoingConnections:
            - output->"Assemble Prompt" 92_icEvqGJNAYqfRkKrnr/message2
          visualData: 2809.218422791587/1084.5003578730216/250/96
        '[W7QsTE8PujofjTwS_4-rK]:text "Text"':
          data:
            text: >-
              No system command found!


              You must restate your entire last message, but it must be in this YAML format:


              ```yaml

              systemCommands:
                - name: COMMAND_NAME
                  arguments:
                  argumentName: argument value
              ``
          outgoingConnections:
            - output->"If/Else" f5fMuI0pWIHCyvx9aU4ZU/false
          visualData: 1971.9655784724882/1350.7092231728611/300/99
        '[_t1LKZtmlsPEVBEMZB7NO]:text "Text"':
          data:
            text: "false"
          outgoingConnections:
            - output->"If/Else" m53Fm7rEKyAQRqIg7RL0Y/true
          visualData: 2292.698006415443/797.5445709973859/300/92
        '[f5fMuI0pWIHCyvx9aU4ZU]:ifElse "If/Else"':
          outgoingConnections:
            - output->"Prompt" SoW97Q838zltZj6QFkfeb/input
          visualData: 2622.697161322423/1060.5873756333851/125/95
        '[m53Fm7rEKyAQRqIg7RL0Y]:ifElse "If/Else"':
          outgoingConnections:
            - output->"Loop Controller" 2_KA9pGsv83JjkPd1MV4q/continue
          visualData: 2686.4651139614534/728.9940219104286/125/85
        '[memzsvTFav2ipoBX9uSM5]:graphInput "Graph Input"':
          data:
            dataType: chat-message[]
            id: prompt
          outgoingConnections:
            - data->"Loop Controller" 2_KA9pGsv83JjkPd1MV4q/input1Default
          visualData: 1088.5984676503394/847.4338448982371/300/72
        '[q13gYH-nPwuJkwVWm6EuO]:text "Text"':
          data:
            text: ""
          outgoingConnections:
            - output->"If/Else" f5fMuI0pWIHCyvx9aU4ZU/true
          visualData: 2162.480754304652/1105.0577958952597/300/100
        '[wkQn6ctmCI1yXdo6sGGBz]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: response
          visualData: 2224.554815375056/290.04076920266385/300/101
    yoe9VPnjUULQacHSolgiL:
      metadata:
        description: ""
        id: yoe9VPnjUULQacHSolgiL
        name: Execute Task List
      nodes:
        '[0bSFyrwELYC6bRERr5Znc]:extractObjectPath "Remaining Tasks"':
          data:
            path: $.yamlDocument.remainingTasks[*]
            usePathInput: false
          outgoingConnections:
            - all_matches->"Remaining Task" mOqA0ixaGWn4vW2dVbGu7/input
          visualData: 8858.792221881899/-10.430405764192535/250/302
        '[0xxgB-83ezLpsw0rLpo86]:subGraph "Get Rivet File By File Name"':
          data:
            graphId: JcFUPKbbvOvBQYdvItenL
          outgoingConnections:
            - file_contents->"Text" i4cBHxWO1HLsnHg3Yh7qP/file_contents
          visualData: 5887.468401346239/-209.5399625226982/300/269
        '[16UsT_tJlpYi717REkv6n]:assemblePrompt "Assemble Prompt"':
          outgoingConnections:
            - prompt->"Chat" F3cvl2CAcA0PAj6KEiF6q/prompt
          visualData: 1158/478/250/11
        '[18WP87xc4D7JXdSGTRlYD]:graphInput "Graph Input"':
          data:
            dataType: string
            id: context
            useDefaultValueInput: true
          outgoingConnections:
            - data->"Execute Current Task Prompt" KIFBcqmOa5G-MPklHfHVR/context
            - data->"Text" uEWXLi8XmKpaiGzzTFDme/context
          visualData: 2071.0798553565564/161.51608679287446/300/328
        '[2duN-SQvGKbhGgvdS0U6n]:text "Text"':
          data:
            text: >-
              Here is a task list provided by another AI:


              """

              {{task_list}}

              """


              Your current task is: No task


              If your current task is complete, what is your next task? Please reply with a short key or sentence describing your next task to execute.


              Reply in this format:


              Next Task: A short description of what I will be doing next
          outgoingConnections:
            - output->"Assemble Prompt" 16UsT_tJlpYi717REkv6n/message2
          visualData: 779.0555310294967/440.4303654785995/300/39
        '[3bvstHODyToviHmuGwEl0]:text "Text"':
          data:
            text: "- {{input}}"
          outgoingConnections:
            - output->"Task List" tgcfyWHl6ut3-ivyZ8jEA/input
          visualData: 864.2677686086065/869.4863033502647/132.05028524130944/null
        '[3uXtY4n-rlV41w6joxUdE]:extractObjectPath "Extract Object Path"':
          data:
            path: $.message
            usePathInput: false
          outgoingConnections:
            - match->"User Input" RIiW55iFfMEKP4fXdsETJ/questions
          visualData: 5570.698610051417/-729.7916844518388/250/355
        '[3xo9qKcGwicvWvyQkgZD4]:extractYaml "Extract YAML"':
          data:
            rootPropertyName: yamlDocument
          outgoingConnections:
            - output->"Next Task" xP7TQ0W3vIsCOyGtZEX5q/object
            - output->"Remaining Tasks" 0bSFyrwELYC6bRERr5Znc/object
          visualData: 8587.35236611393/-174.5117304558507/134.44348359916876/299
        '[4CgGTnsYZdZFKi12cKmiP]:if "If"':
          outgoingConnections:
            - output->"Text" FyLqlQhLYBHb5SD8Bbjx5/input
          visualData: 5752.306145236096/217.18316729256162/100/362
        '[4JUAkBJxsbKfNAXOdmGS6]:extractObjectPath "Extract Object Path"':
          data:
            path: $.notes
            usePathInput: false
          outgoingConnections:
            - match->"Coalesce" SNJjOp-rPmQwnU52nE8kO/input7
          visualData: 5874.8418618563355/7.449881473971996/250/377
        '[4S1D75ofp0W1obD11VBqI]:subGraph "List Rivet Files"':
          data:
            graphId: d6Pgmz7n8qvXkaNF-2e9P
          outgoingConnections:
            - files->"Execute Current Task Prompt" KIFBcqmOa5G-MPklHfHVR/files
          visualData: 3119.481362252188/-57.27713878661959/300/260
        '[6P04DpTqBQMOYpBfJuurJ]:text "Text"':
          data:
            text: |-
              
              SYSTEM_MESSAGE:

              You ran this command:

              ```yaml
              command: {{command_name}}
              arguments: {{arguments}}
              ```

              The output of this command is:

              COMMAND_OUTPUT_START
              {{command_output}}
              COMMAND_OUTPUT_END
          outgoingConnections:
            - output->"Command History" JA8Lk0pXQTMaNfUhEphm2/command
            - output->"Text" uEWXLi8XmKpaiGzzTFDme/command_info
          visualData: 7486.925742104204/-160.8828855686792/300/229
        '[8xAF_gsGq9anwwpehGo2Y]:text "Text"':
          data:
            text: >-
              You have Chat API functions available to you to use. You may call
              one of these functions to get a response from the system:


              - command: READ_FILES
                arguments:
                  files:
                    - file_name.ts
                description: Reads a file and returns with its contents. Files must be an array with one file per line.
              - command: TAKE_NOTE_FOR_SELF
                arguments:
                  notes: Here is some note to take for youself.
              - command: THINK_OUT_LOUD
                arguments:
                  notes: Here is some text to remember for later.
              - command: WRITE_FILE
                arguments:
                  file: file_name.ts
                contents: |
                  contents of the file
                description: Writes text to the specified file. You must write the entire file contents at once. You must not comment out sections of the file - everything needs to be implemented.
              - command: ASK_FOR_FEEDBACK
                arguments:
                  message: The message for the user
                description: Asks the user questions or for feedback on what you are doing. This can help make sure your plan is good.

              An example is:


              ```yaml

              yamlDocument:
                command: THINK_OUT_LOUD
                arguments:
                  notes: This is me thinking out loud
              ```
          outgoingConnections:
            - output->"Execute Current Task Prompt"
              KIFBcqmOa5G-MPklHfHVR/functions
            - output->"Text" uEWXLi8XmKpaiGzzTFDme/functions
          visualData: 2432.601813980837/872.2598561008039/300/160
        '[B0u3WpTZggnq8Lgl7Xv8h]:text "Text"':
          data:
            text: >-
              Here is some text about a task list:


              """

              {{text}}

              """


              Convert this text into a YAML document with the following format:


              ```yaml

              yamlDocument:
                nextTask: The next task I should execute
                remainingTasks:
                  - The next task remaining
                  - Another task remaining
              ```

              Every task MUST be a string value, even if the step contains YAML data.
          outgoingConnections:
            - output->"Chat" PLU5yC3u52dOPIxlmPIf-/prompt
          visualData: 8226.95339201243/-548.2113674897184/300/290
        '[CGAZhTY_eFuZpMhTT_-XX]:text "Text"':
          data:
            text: You are a AI tool that analyzes another AI's message and determines the
              next task for it to run.
          outgoingConnections:
            - output->"Chat" gHSSvkoWQ76ko-0EvUdZn/systemPrompt
          visualData: 7832.703318346902/-435.1156370653609/300/289
        '[CWz3hh4xEp4FLbYV3Tw3v]:if "If"':
          outgoingConnections:
            - output->"Extract Object Path" wq5E83vpWqQMlS7AHPnQe/object
          visualData: 5388.238498672117/-404.8323310274315/100/346
        '[F3cvl2CAcA0PAj6KEiF6q]:chat "Chat"':
          data:
            cache: false
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0
            stop: ""
            temperature: 0.5
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          outgoingConnections:
            - response->"Extract Regex" L8j7UVv6-Qc2Pnm9B4MRB/input
          visualData: 1478/413/200/13
        '[FyLqlQhLYBHb5SD8Bbjx5]:text "Text"':
          data:
            text: |-
              SYSTEM ERROR: NO SUCH FUNCTION

              INPUT:

              ```yml
              {{input}}
              ```
          outgoingConnections:
            - output->"Coalesce" SNJjOp-rPmQwnU52nE8kO/input6
          visualData: 6421.102213946726/101.64363655647925/300/374
        '[H4TGM34XgUZZOny6PfmFt]:text "Remaining Tasks"':
          data:
            text: "{{input}}"
          outgoingConnections:
            - output->"Loop Controller" cObRObIxgwrJablQHLGnw/input2
          visualData: 9376.061220068554/4.762078225718085/154.63565140719766/326
        '[ItFByM3wvP_dPEoP5D6Mx]:if "If"':
          outgoingConnections:
            - output->"Coalesce" YJDAqTtxqpNkKPX5FvtPt/input2
          visualData: 5380.482613085664/157.3401052405825/100/373
        '[JA8Lk0pXQTMaNfUhEphm2]:text "Command History"':
          data:
            text: |-
              {{prev_commands}}
              {{command}}
          outgoingConnections:
            - output->"Loop Controller" cObRObIxgwrJablQHLGnw/input3
          visualData: 7911.318384602866/322.2310779162274/300/382
        '[KIFBcqmOa5G-MPklHfHVR]:text "Execute Current Task Prompt"':
          data:
            text: >-
              {{context}}


              Here are all files in the project:


              FILES_START

              {{files}}

              FILES_END


              {{functions}}


              Here is a history of your commands:


              HISTORY_START

              {{previously_executed}}

              HISTORY_END


              Here are your remaining tasks:


              - {{current_task}}

              {{task_list}}


              Reply with the Chat API function you will run next to further your task list. Primarily, focus on executing the first task(s) in your list. Do not run commands that have already ran before.



              Reply with a YAML document explaining your command, like this:


              ```yaml

              yamlDocument:
                currentTask: Restate your current task from the task list
                reasonForCommand: Explain the reason for choosing the command and how it will help execute your task list.
                command: COMMAND_NAME
                arguments:
                  argumentName: value
                  argument2: value2
              ```
          outgoingConnections:
            - output->"Chat - Execute Current Task" NlqmTc5SUrwdCFOOU26CP/prompt
          visualData: 3827.4091396369313/31.86885376256144/300/215
        '[L27NwFvq6cJgke83F55ei]:userInput "User Input"':
          data:
            prompt: This is an example question?
            useInput: true
          outgoingConnections:
            - output->"Coalesce" SNJjOp-rPmQwnU52nE8kO/input2
          visualData: 6271.2758460845735/-1025.009768641542/250/254
        '[L8j7UVv6-Qc2Pnm9B4MRB]:extractRegex "Extract Regex"':
          data:
            errorOnFailed: false
            regex: "Next Task: (.+)"
            useRegexInput: false
          outgoingConnections:
            - output1->"Loop Controller" cObRObIxgwrJablQHLGnw/input1Default
          visualData: 1734/445/145/22
        '[MHz_Mr-O-KivwLl_Qbevp]:text "Text"':
          data:
            text: >-
              yamlDocument:
                steps:
                  - First, I will review the requirements and specifications for the TrimChatMessagesNode component to ensure that I understand what is expected of me.
                  - Next, I will review the existing codebase to gain an understanding of how the system works and how the TrimChatMessagesNode component fits into it.
                  - I will examine the ExtractRegexNode.tsx file to see how a similar component is implemented and determine if there are any useful patterns or techniques that can be applied to the TrimChatMessagesNode component.
                  - I will create a new file called TrimChatMessagesNode.tsx and implement the components according to the specifications and examples provided.
          outgoingConnections:
            - output->"Extract YAML" bjL5VE8Zzz84m3QFLJ7x7/input
          visualData: -870.9610414764056/564.0734459241355/300/274
        '[Mvbq2URURhELcYZH2EUKw]:if "If"':
          outgoingConnections:
            - output->"Extract Object Path" 3uXtY4n-rlV41w6joxUdE/object
          visualData: 5375.549822945474/-705.1813531485337/100/351
        '[NlqmTc5SUrwdCFOOU26CP]:chat "Chat - Execute Current Task"':
          data:
            cache: false
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-4
            presencePenalty: 0
            stop: ""
            temperature: 0.2
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          outgoingConnections:
            - response->"Extract YAML" RRQkzAYP86x8a9_9_KOx_/input
            - response->"If" 4CgGTnsYZdZFKi12cKmiP/value
          visualData: 4196.338904061931/191.56471881852312/194.08208299619673/278
        '[PLU5yC3u52dOPIxlmPIf-]:chat "Chat"':
          data:
            cache: true
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0
            stop: ""
            temperature: 0
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          outgoingConnections:
            - response->"Extract YAML" 3xo9qKcGwicvWvyQkgZD4/input
          visualData: 8606.039472972514/-393.373390759543/200/292
        '[Q2-bYh6LGkVm2WPb46tEz]:text "Text"':
          data:
            text: "Please create this file: {{file_path}}"
          outgoingConnections:
            - output->"User Input" L27NwFvq6cJgke83F55ei/questions
          visualData: 5885.144058035461/-1031.3397979538224/300/251
        '[RIiW55iFfMEKP4fXdsETJ]:userInput "User Input"':
          data:
            prompt: This is an example question?
            useInput: true
          outgoingConnections:
            - output->"Coalesce" SNJjOp-rPmQwnU52nE8kO/input3
          visualData: 5907.5865905173505/-741.4244278640571/250/259
        '[RNxCaWdA7mOcrJBVUKiqr]:if "If"':
          outgoingConnections:
            - output->"Extract Object Path" iAPKowmhorGhT6DKcjkb7/object
          visualData: 5389.610793391964/-187.54344359294475/100/368
        '[RRQkzAYP86x8a9_9_KOx_]:extractYaml "Extract YAML"':
          data:
            objectPath: $.yamlDocument
            rootPropertyName: yamlDocument
          outgoingConnections:
            - output->"Extract Object Path" TflHhIRIPHK_PlpMpMCtL/object
            - output->"Extract Object Path" z5IaklOOBA20wwMI0aRkq/object
          visualData: 4198.2280237908735/436.0668212402672/149.28242186462194/281
        '[Rh5FbKt-FMnow6je0Hm4u]:if "If"':
          outgoingConnections:
            - output->"Extract Object Path" u2sddIBdrSCM-8EkJ_qS4/object
          visualData: 5391.813974291001/-998.058677960689/100/358
        '[SNJjOp-rPmQwnU52nE8kO]:coalesce "Coalesce"':
          outgoingConnections:
            - output->"Text" 6P04DpTqBQMOYpBfJuurJ/command_output
          visualData: 7107.0327313666685/-155.45997396385604/150/285
        '[SNQCA2ZXN7LQRbPd1M6Mx]:subGraph "Get Rivet File By File Name"':
          data:
            graphId: JcFUPKbbvOvBQYdvItenL
          outgoingConnections:
            - file_contents->"Digest File" cV8OdlEuJDaI2v05bTz5X/file_contents
          visualData: 5899.625587751349/-459.07944232089864/300/267
        '[TGIDciqmI5zMTLdVk2T2C]:if "If"':
          outgoingConnections:
            - output->"Coalesce" YJDAqTtxqpNkKPX5FvtPt/input1
          visualData: 5396.161154674258/9.417887736214652/100/367
        '[TflHhIRIPHK_PlpMpMCtL]:extractObjectPath "Extract Object Path"':
          data:
            path: $.command
            usePathInput: false
          outgoingConnections:
            - match->"Match" eKe_lJayfu9fVyZAD7ri_/input
            - match->"Text" 6P04DpTqBQMOYpBfJuurJ/command_name
          visualData: 4437.355919239172/355.562416243158/250/372
        '[U0EcCqP66WFZahvpiGPFf]:userInput "User Input"':
          data:
            prompt: This is an example question?
            useInput: true
          outgoingConnections:
            - output->"Coalesce" SNJjOp-rPmQwnU52nE8kO/input1
          visualData: 6281.8258949383735/-1270.8259069351022/250/256
        '[VIZaMd_lS9i-zMjpKl5wj]:text "Task List"':
          data:
            text: "{{input}}"
          outgoingConnections:
            - output->"Text" uEWXLi8XmKpaiGzzTFDme/task_list
          visualData: 5995.575145528108/766.7859412690377/280.41157562862554/384
        '[Vudek72LbIFekPR5DVDsq]:graphInput "Task List"':
          data:
            dataType: string[]
            id: task_list
            useDefaultValueInput: true
          outgoingConnections:
            - data->"Text" 2duN-SQvGKbhGgvdS0U6n/task_list
            - data->"Text" 3bvstHODyToviHmuGwEl0/input
          visualData: 424/642/300/4
        '[YDjSbuZX0TA_4lFNEQGaN]:text "Text"':
          data:
            text: You are an AI that converts English text into YAML documents, given a
              structure to output in.
          outgoingConnections:
            - output->"Chat" PLU5yC3u52dOPIxlmPIf-/systemPrompt
          visualData: 8217.342758974006/-728.677698989026/300/294
        '[YJDAqTtxqpNkKPX5FvtPt]:coalesce "Coalesce"':
          outgoingConnections:
            - output->"Extract Object Path" 4JUAkBJxsbKfNAXOdmGS6/object
          visualData: 5664.504228276922/38.811399688325125/150/376
        '[YMPhS3MhQdfMqPqj8Ss8k]:text "Text"':
          data:
            text: |-
              Please write these contents to {{file_path}}:

              {{contents}}
          outgoingConnections:
            - output->"User Input" U0EcCqP66WFZahvpiGPFf/questions
          visualData: 5903.034048264701/-1281.4210486111251/300/255
        '[_tOy610lqFqHX6iUfQZu1]:extractObjectPath "Extract Object Path"':
          data:
            path: $.file
            usePathInput: false
          outgoingConnections:
            - match->"Text" YMPhS3MhQdfMqPqj8Ss8k/file_path
          visualData: 5570.538237015827/-1264.1784150771389/250/361
        '[aymg3sOX3k4VaMI8L7NUg]:text "Text"':
          data:
            text: >
              Here is additional context:


              I am working on developing an AI storyboarding tool that allows users to create a series of prompts for a language model in a choose-your-own-adventure format. The tool is inspired by node-based editors, like the one found in Blender, where users can create nodes on a page that have inputs and outputs that can be connected by wires to form a web of connections between the prompts and the AI. Each node can be edited, and when editing, a larger window pops up with a text editor where users can tweak various aspects of the prompt that will be fed to the AI. This tool will provide a user-friendly interface for crafting interactive stories with an AI language model. Here is a tree of my current files for context. If you would like the contents of any of these files, please ask. The app is dark-themed and the colors are available in index.css. I'm using Emotion for CSS.


              I have asked this question:


              """

              I need to implement the TrimChatMessagesNode.tsx file for the TrimChatMessages.ts file.

              """


              Here are some additional notes:


              Can you provide more information on the expected behavior of the TrimChatMessagesNode component?

              It takes in an chat-message[] and cuts messages off the beginning or end until it reaches a specified token count.

              Are there any specific styling requirements for this component?

              No

              How does the TrimChatMessagesNode component interact with the rest of the system?

              Same as everything else

              Are there any existing tests for the TrimChatMessagesNode component that I should be aware of?

              No

              Is there any documentation or examples available for similar components that I can reference while implementing this?

              A good example would be ExtractRegexNode.ts
          outgoingConnections:
            - output->"Graph Input" 18WP87xc4D7JXdSGTRlYD/default
          visualData: 1596.3955567814364/-106.05238101849785/300/35
        '[bjL5VE8Zzz84m3QFLJ7x7]:extractYaml "Extract YAML"':
          data:
            rootPropertyName: yamlDocument
          outgoingConnections:
            - output->"Extract Object Path" qt2AdWZptLCfvACUeDtea/object
          visualData: -511.4509344968344/638.2666631869166/250/275
        '[cER_B2LLcZYy15z-KNJDK]:prompt "Prompt"':
          data:
            promptText: You are a programming assistant that iteratively executes commands
              and thinks out loud to accomplish tasks.
            type: system
            useTypeInput: false
          outgoingConnections:
            - output->"Assemble Prompt" 16UsT_tJlpYi717REkv6n/message1
            - output->"System" z1APvC5m5fW-AIfzoVeox/input
          visualData: 553.1915381834611/-17.88559476470175/null/28
        '[cObRObIxgwrJablQHLGnw]:loopController "Loop Controller"':
          outgoingConnections:
            - break->"Graph Output" xe4vk7BUOsT-mnIoEjOha/value
            - output1->"Execute Current Task Prompt"
              KIFBcqmOa5G-MPklHfHVR/current_task
            - output1->"Text" uEWXLi8XmKpaiGzzTFDme/current_task
            - output2->"Execute Current Task Prompt"
              KIFBcqmOa5G-MPklHfHVR/task_list
            - output2->"Task List" VIZaMd_lS9i-zMjpKl5wj/input
            - output3->"Command History" JA8Lk0pXQTMaNfUhEphm2/prev_commands
            - output3->"Execute Current Task Prompt"
              KIFBcqmOa5G-MPklHfHVR/previously_executed
          visualData: 2012.432896034012/500.84246666707406/439.1093458908531/133
        '[cV8OdlEuJDaI2v05bTz5X]:subGraph "Digest File"':
          data:
            graphId: HXjZhpWO0hluMiDY6pneE
          outgoingConnections:
            - digest->"Coalesce" SNJjOp-rPmQwnU52nE8kO/input4
          visualData: 6325.94927766123/-439.4589639215074/300/130
        '[dHDzOydr6Q5IFDBH6z6HU]:if "If"':
          outgoingConnections:
            - output->"Extract Object Path" _tOy610lqFqHX6iUfQZu1/object
          visualData: 5368.851714764555/-1237.700159446597/100/359
        '[eKe_lJayfu9fVyZAD7ri_]:match "Match"':
          data:
            caseCount: 7
            cases:
              - DIGEST_FILE
              - READ_FILES
              - TAKE_NOTE_FOR_SELF
              - ASK_FOR_FEEDBACK
              - CREATE_BLANK_FILE
              - WRITE_FILE
              - THINK_OUT_LOUD
          outgoingConnections:
            - case1->"If" CWz3hh4xEp4FLbYV3Tw3v/if
            - case2->"If" RNxCaWdA7mOcrJBVUKiqr/if
            - case3->"If" TGIDciqmI5zMTLdVk2T2C/if
            - case4->"If" Mvbq2URURhELcYZH2EUKw/if
            - case5->"If" Rh5FbKt-FMnow6je0Hm4u/if
            - case6->"If" dHDzOydr6Q5IFDBH6z6HU/if
            - case7->"If" ItFByM3wvP_dPEoP5D6Mx/if
            - unmatched->"If" 4CgGTnsYZdZFKi12cKmiP/if
          visualData: 4151.319415255813/-535.8389204523792/300/334
        '[gHSSvkoWQ76ko-0EvUdZn]:chat "Chat"':
          data:
            cache: false
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-4
            presencePenalty: 0
            stop: ""
            temperature: 0.3
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          outgoingConnections:
            - response->"Text" B0u3WpTZggnq8Lgl7Xv8h/text
          visualData: 8224.64735040648/-110.60521742736663/200/288
        '[i4cBHxWO1HLsnHg3Yh7qP]:text "Text"':
          data:
            text: |-
              ```
              // {{file_name}}
              {{file_contents}}
              ```
          outgoingConnections:
            - output->"Coalesce" SNJjOp-rPmQwnU52nE8kO/input5
          visualData: 6300.416322248281/-178.5223306236974/300/272
        '[iAPKowmhorGhT6DKcjkb7]:extractObjectPath "Extract Object Path"':
          data:
            path: $.files[*]
            usePathInput: false
          outgoingConnections:
            - all_matches->"Get Rivet File By File Name"
              0xxgB-83ezLpsw0rLpo86/fileMatch
            - all_matches->"Text" i4cBHxWO1HLsnHg3Yh7qP/file_name
          visualData: 5557.238871649446/-193.98449142035096/250/337
        '[mOqA0ixaGWn4vW2dVbGu7]:text "Remaining Task"':
          data:
            text: "- {{input}}"
          outgoingConnections:
            - output->"Remaining Tasks" H4TGM34XgUZZOny6PfmFt/input
          visualData: 9157.914625813317/9.440688943435534/176.6780763504812/315
        '[qt2AdWZptLCfvACUeDtea]:extractObjectPath "Extract Object Path"':
          data:
            path: $.yamlDocument.steps
            usePathInput: false
          outgoingConnections:
            - match->"Task List" Vudek72LbIFekPR5DVDsq/default
          visualData: -33.58278249236042/631.2095385792555/250/277
        '[tgcfyWHl6ut3-ivyZ8jEA]:text "Task List"':
          data:
            text: "{{input}}"
          outgoingConnections:
            - output->"Loop Controller" cObRObIxgwrJablQHLGnw/input2Default
          visualData: 1045.9677524118683/852.789548081857/116.3356920475137/370
        '[u2sddIBdrSCM-8EkJ_qS4]:extractObjectPath "Extract Object Path"':
          data:
            path: $.file
            usePathInput: false
          outgoingConnections:
            - match->"Text" Q2-bYh6LGkVm2WPb46tEz/file_path
          visualData: 5550.538237015827/-1026.4048579184675/250/357
        '[uEWXLi8XmKpaiGzzTFDme]:text "Text"':
          data:
            text: >-
              {{context}}


              {{functions}}


              Your current task was "{{current_task}}" and you ran this command to fulfill it:


              {{command_info}}


              Here are your remaining tasks:


              {{task_list}}


              Does your last command ran above finish your current task "{{current_task}}"?


              List the tasks that your last command completed.


              Next, imagine a scenario where you are an expert in the Rivet project, would they consider your current task completed? Would they consider the remaining tasks still applicable? Here are some examples of what an expert might ask:


              "If you need examples, do you have enough? Or would more be better?"

              "If you have to guess about anything, would looking at any other files help?"

              "Have you taken enough notes for yourself?"

              "You could combine these steps"

              "This step sounds redundant"


              Write down what the expert would say or ask you about your current task list.


              Next, answer the expert's questions or reply to their thoughts.


              Next, revise your task list (remove your now completed tasks), explain your revisions, and write down your current task.
          outgoingConnections:
            - output->"Chat" gHSSvkoWQ76ko-0EvUdZn/prompt
          visualData: 7875.686049522404/-255.33900115338668/300/381
        '[wq5E83vpWqQMlS7AHPnQe]:extractObjectPath "Extract Object Path"':
          data:
            path: $.file
            usePathInput: false
          outgoingConnections:
            - match->"Digest File" cV8OdlEuJDaI2v05bTz5X/file_name
            - match->"Get Rivet File By File Name"
              SNQCA2ZXN7LQRbPd1M6Mx/fileMatch
          visualData: 5536.689177597142/-434.1124731488284/250/345
        '[xP7TQ0W3vIsCOyGtZEX5q]:extractObjectPath "Next Task"':
          data:
            path: $.yamlDocument.nextTask
            usePathInput: false
          outgoingConnections:
            - match->"Loop Controller" cObRObIxgwrJablQHLGnw/input1
          visualData: 8854.274134954136/-180.79168735899/250/311
        '[xe4vk7BUOsT-mnIoEjOha]:graphOutput "Graph Output"':
          data:
            dataType: string
            id: output
          visualData: 2652.651717877761/223.07146808148678/300/161
        '[z1APvC5m5fW-AIfzoVeox]:text "System"':
          data:
            text: "{{input}}"
          outgoingConnections:
            - output->"Chat - Execute Current Task"
              NlqmTc5SUrwdCFOOU26CP/systemPrompt
          visualData: 3617.485828731137/-47.72596476201433/148.77673982924148/279
        '[z5IaklOOBA20wwMI0aRkq]:extractObjectPath "Extract Object Path"':
          data:
            path: $.arguments
            usePathInput: false
          outgoingConnections:
            - match->"If" CWz3hh4xEp4FLbYV3Tw3v/value
            - match->"If" ItFByM3wvP_dPEoP5D6Mx/value
            - match->"If" Mvbq2URURhELcYZH2EUKw/value
            - match->"If" RNxCaWdA7mOcrJBVUKiqr/value
            - match->"If" Rh5FbKt-FMnow6je0Hm4u/value
            - match->"If" TGIDciqmI5zMTLdVk2T2C/value
            - match->"If" dHDzOydr6Q5IFDBH6z6HU/value
            - match->"Text" 6P04DpTqBQMOYpBfJuurJ/arguments
          visualData: 4434.673893565094/600.9131958907702/250/385
    zYzI7xjOb0NOj9WPKJOrQ:
      metadata:
        id: zYzI7xjOb0NOj9WPKJOrQ
        name: "**Rivet Helper"
      nodes:
        '[1K582UsQEjPvjVRvtf4ag]:userInput "Question"':
          data:
            prompt: What is your question for the AI to answer?
            useInput: false
          outgoingConnections:
            - output->"Prompt Question List" 5-z53LCARmASeWft67PEH/request
            - output->"Prompt Question List" KbotaIf5SLrUkOubRve1x/request
            - output->"Prompt Question List" eeqzpCkSxMDTin0sOghhU/request
            - output->"Prompt" YFPEXiTSWNawBUvgAF0_F/question
          visualData: -2195.2160232826423/-17.22315339809552/378.6378084183709/537
        '[2AsgAl-VGTQHHKgMYanRs]:subGraph "List Rivet Files"':
          data:
            graphId: d6Pgmz7n8qvXkaNF-2e9P
          outgoingConnections:
            - files->"Prompt Question List" 5-z53LCARmASeWft67PEH/files
            - files->"Prompt Question List" KbotaIf5SLrUkOubRve1x/files
            - files->"Prompt Question List" eeqzpCkSxMDTin0sOghhU/files
          visualData: -2127.682357265696/-240.91939301727035/300/536
        '[4-0zbzUfiK5tL1R22gT2z]:text "Text"':
          data:
            text: >-
              Here are some answers given for some questions:


              """

              {{answers}}

              """


              Here is some required changes you need to make to these answers:


              """

              {{feedback}}

              """


              Rewrite these answers with the changes. Additionally, rephrase the answers as statements instead of answers, as if they were found in a technical document for the project.
          outgoingConnections:
            - output->"Chat" xxSFUqUpaoGhRrU__HAUO/prompt
          visualData: 1216.727187150413/-425.279616238741/300/625
        '[41OQXeM5D_KXd8nvMgD13]:extractYaml "Extract YAML"':
          data:
            objectPath: $.yamlDocument.steps
            rootPropertyName: yamlDocument
          outgoingConnections:
            - output->"Execute Task List" yWAKuzopcX8M17WGEDi-6/task_list
          visualData: 5077.347198549942/142.63332091512157/179.6401800954518/678
        '[5-z53LCARmASeWft67PEH]:prompt "Prompt Question List"':
          data:
            promptText: >-
              {{context}}


              These files are present in the Rivet application:


              {{files}}


              I have the following question or request:


              """

              {{request}}

              """


              You provided me with these questions:


              """

              {{questions}}

              """


              Imagine a scenario where you are the expert on the Rivet application, or a project manager on the project. What are your best answers to these questions? You may say that you do not have enough context to answer any given question. Also, phrase the answers in such a way to include the question in them. Directly answer the questions, as if this is documentation on the project.
            type: user
            useTypeInput: false
          outgoingConnections:
            - output->"Chat" RfEA6xsgL1vEu9oDqRe88/prompt
          visualData: -1488.9069047003868/208.5550898161199/485.03013745499055/546
        '[6UTrt-tWAv0UmymCmjyGg]:if "If"':
          outgoingConnections:
            - output->"Coalesce" b3NT35IXmv6oqd2kSitLM/input2
          visualData: 1111/592/100/596
        '[7vNigNnYPyllzuKCncBQd]:text "Text"':
          data:
            text: >-
              {{context}}


              {{functions}}


              Give me a detailed overview of the steps you will be taking to accomplish this task.


              Make sure you gather context on what you are working with, look at examples, think out loud, give high level tasks, get one or mote examples before creating new files, read files before writing them, and are very detailed.


              You can only use the Chat API to do things like read files and think our loud. You are not working in a repository. There are no branches nor git. You are an AI that is interacting with the system functions only. However, do not include YAML in your response. Your response should be plain English, and may mention the Chat API functions in sentences.


              There are no tests to read. There is no documentation to read. Do not write tests. Do not write documentation.
          outgoingConnections:
            - output->"Chat" wnITtjycZ2FJpLYSThTVo/prompt
          visualData: 2260.208598489028/282.32332571050824/300/677
        '[9f2Us4WxeKYfvqMm1_XUZ]:userInput "User Input"':
          data:
            prompt: This is an example question?
            useInput: true
          outgoingConnections:
            - questionsAndAnswers->"Coalesce" b3NT35IXmv6oqd2kSitLM/input1
          visualData: 1168.2615368939803/321.2573005264068/187/594
        '[AGmIpXCDl_sHyhLIjtXie]:chat "Get Question List"':
          data:
            cache: true
            maxTokens: 1024
            model: gpt-3.5-turbo
            temperature: 0
            top_p: 1
            useMaxTokensInput: false
            useModelInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          outgoingConnections:
            - response->"Prompt Question List" 5-z53LCARmASeWft67PEH/questions
            - response->"Text" VszyH3-ShGM2DBZw1v7VW/questions
          visualData: -885.2318474821926/-230.54556081772057/233.53671241371717/544
        '[At8pw5kk0skpMBKEBPgZp]:chat "Chat"':
          data:
            cache: true
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0
            stop: ""
            temperature: 0
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          outgoingConnections:
            - response->"Extract YAML" 41OQXeM5D_KXd8nvMgD13/input
          visualData: 4811.227708168585/182.57139818613453/200/658
        '[B7bKOsVis46f2wZGjHok0]:chat "Chat"':
          data:
            cache: true
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0
            stop: ""
            temperature: 0
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          outgoingConnections:
            - response->"Text" oFk-Eg5dbpoWQCK9hx_gs/steps
            - response->"Text" sMP1SZoQAgvMH3UtERway/steps
          visualData: 3464.179659645404/24.612441249477307/200/661
        '[CQ11jAUax-dtEvf7Jw3Ux]:if "If"':
          outgoingConnections:
            - output->"User Input" 9f2Us4WxeKYfvqMm1_XUZ/questions
          visualData: 1044/328/100/593
        '[CZqk_fQFmbzNKep6CsIVU]:text "Text"':
          data:
            text: You are an advanced AI that makes changes to text.
          outgoingConnections:
            - output->"Chat" xxSFUqUpaoGhRrU__HAUO/systemPrompt
          visualData: 1188.9614185009382/-574.3368036976119/300/null
        '[CfsTEn_pniYVjsyJP-paR]:text "Text"':
          data:
            text: "{{context}}"
          outgoingConnections:
            - output->"Assemble Prompt" H7i41CNQTM5fM5KcJDwfq/message1
            - output->"Execute Task List" yWAKuzopcX8M17WGEDi-6/context
          visualData: 2657.4058456180655/-200.8621114414035/300/643
        '[DYYQPYCdQyMP2w5Bh2QOO]:text "Text"':
          data:
            text: >-
              Here is a list of steps:


              """

              {{steps}}

              """


              Convert this list of steps into a YAML document with the following format:


              ```yaml

              yamlDocument:
                steps:
                  - Step 1
                  - Step 2
              ```


              You must include every single step. Every step MUST be a string, even if the list of steps contains YAML data.
          outgoingConnections:
            - output->"Chat" At8pw5kk0skpMBKEBPgZp/prompt
          visualData: 4482.313290057272/41.00054812622696/300/660
        '[GsiNZWC4KFQa-MiYQcjUG]:extractObjectPath "Extract Object Path"':
          data:
            path: $.yamlDocument.questions
            usePathInput: false
          outgoingConnections:
            - match->"If" CQ11jAUax-dtEvf7Jw3Ux/value
          visualData: 627.8152048842937/679.5426701770796/227/583
        '[H7i41CNQTM5fM5KcJDwfq]:assemblePrompt "Assemble Prompt"':
          outgoingConnections:
            - prompt->"Chat" B7bKOsVis46f2wZGjHok0/prompt
          visualData: 3184.1516408542043/114.06032561384635/250/642
        '[Hqpwl6OBs8GdL917iMJNw]:extractObjectPath "Extract Object Path"':
          data:
            path: $.yamlDocument[*].question
            usePathInput: false
          visualData: 725.0975654729407/-223.09365273391288/205.2941012713909/610
        '[KbotaIf5SLrUkOubRve1x]:prompt "Prompt Question List"':
          data:
            promptText: >-
              {{context}}


              These files are present in the Rivet application:


              {{files}}


              I have the following question or request:


              """

              {{request}}

              """


              Imagine that you are a developer assigned to this task on the project. You are already aware of how all files in the project already work. But you are likely missing some information to fully complete the request. Please give a list of questions that you would ask the project manager or another developer. Examples are clarifying behavior, or styling, or interaction with the rest of the system, etc.


              You have a maximum of 6 questions.
            type: user
            useTypeInput: false
          outgoingConnections:
            - output->"Get Question List" AGmIpXCDl_sHyhLIjtXie/message2
            - output->"Get Question List" AGmIpXCDl_sHyhLIjtXie/prompt
            - output->"Prompt Question List" eeqzpCkSxMDTin0sOghhU/questions
          visualData: -1490.4278366754174/-424.16270749159196/485.03013745499055/538
        '[NCeGFcpBOCqnZICq3vn9m]:extractObjectPath "Extract Object Path"':
          data:
            path: $.yamlDocument['has-questions']
            usePathInput: false
          outgoingConnections:
            - match->"Match" kshUlDyeosoaczIy5rp_C/input
          visualData: 624.8152048842937/494.54267017707963/172.91706398583165/578
        '[P3JViEfh4tcb4Kq8X9TSo]:text "Text"':
          data:
            text: You are an advanced AI YAML generator tool that takes in inputs and
              produces valid YAML based on a question.
          outgoingConnections:
            - output->"Chat" noeONFrQRJhnNOIqzfe0v/systemPrompt
            - output->"Chat" wTZ8HUHOjU5COaWxaohVb/systemPrompt
          visualData: 30.111429314528237/-82.36315841333024/300/567
        '[RfEA6xsgL1vEu9oDqRe88]:chat "Chat"':
          data:
            cache: true
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-4
            presencePenalty: 0
            stop: ""
            temperature: 0
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          outgoingConnections:
            - response->"Prompt Question List" eeqzpCkSxMDTin0sOghhU/answers
            - response->"Text" VszyH3-ShGM2DBZw1v7VW/answers
          visualData: -860.5478300364443/314.275559094539/200/555
        '[U0bz-jzJ615Vgm78Lpley]:extractYaml "Extract YAML"':
          data:
            rootPropertyName: yamlDocument
          outgoingConnections:
            - output->"Extract Object Path" GsiNZWC4KFQa-MiYQcjUG/object
            - output->"Extract Object Path" NCeGFcpBOCqnZICq3vn9m/object
          visualData: 466.5038997695302/505.5426701770796/137.3689875131763/577
        '[V1Gia2nRcaIx-6Zu8YKjx]:userInput "User Input"':
          data:
            prompt: This is an example question?
            useInput: true
          outgoingConnections:
            - output->"Text" 4-0zbzUfiK5tL1R22gT2z/feedback
          visualData: 1683.6344428877155/-62.36167594296539/173.2418484290331/630
        '[VrlVJ3R4e5XeyDKqHmrjz]:text "Text"':
          data:
            text: >-
              Make these changes:


              * Combine steps that are talking about the same file

              * Add steps to look at examples before any steps that write files

              * Remove any steps talking about documentation

              * Remove any steps talking about testing or tests

              * Flatten any nested steps, there should only be one single list, no nested lists.
          outgoingConnections:
            - output->"Assemble Prompt" H7i41CNQTM5fM5KcJDwfq/message3
          visualData: 2739.943217152992/332.8979395249134/300/641
        '[VszyH3-ShGM2DBZw1v7VW]:text "Text"':
          data:
            text: >-
              Can you please combine these questions and answers into a YAML
              document like this:


              ```yaml

              yamlDocument:
                - question: This is a question
                  answer: This is the answer to the question
                - question: This is a question
                  answer: This is the answer to the question
              ```


              Questions:


              {{questions}}


              Answers:


              {{answers}}



              More Questions and Answers:


              {{qa2}}
          outgoingConnections:
            - output->"Chat" wTZ8HUHOjU5COaWxaohVb/prompt
          visualData: 523.5852096259869/-685.6221670073336/300/607
        '[Y5dFBHDnIc0yTEcNA87W3]:text "Text"':
          data:
            text: |
              {{input}}
          outgoingConnections:
            - output->"Text" pqxbw_BaWOjpkd3G8WnRT/input
          visualData: 976.8681178648283/-36.7432284340459/128.14831452392036/680
        '[YFPEXiTSWNawBUvgAF0_F]:prompt "Prompt"':
          data:
            promptText: |-
              {{context}}

              I have asked this question:

              """
              {{question}}
              """

              Here are some additional notes:

              {{qanda}}
            type: user
            useTypeInput: false
          outgoingConnections:
            - output->"Text" 7vNigNnYPyllzuKCncBQd/context
            - output->"Text" CfsTEn_pniYVjsyJP-paR/context
          visualData: 1879.8025484562081/-164.03783773403975/null/674
        '[Z115WdCClukxy33gFZQLm]:userInput "User Input"':
          data:
            prompt: This is an example question?
            useInput: true
          outgoingConnections:
            - output->"Text" sMP1SZoQAgvMH3UtERway/feedback
          visualData: 3161.412174129719/424.40649291776276/250/668
        '[ZGfDzvqbUcnQunNNplfBA]:text "Text"':
          data:
            text: |-
              Do these answers look sufficient to you? Any changes to make?

              {{answers}}
          outgoingConnections:
            - output->"User Input" V1Gia2nRcaIx-6Zu8YKjx/questions
          visualData: 1397.9960111510754/-66.32861624841605/225.89983857684933/631
        '[_7tr8cgQEbY6mtIakJd-N]:text "Text"':
          data:
            text: "You are an advanced automated code developer. You are able to
              autonomously be given a task, and do all relevant development
              tasks assocated with accomplishing that task. "
          outgoingConnections:
            - output->"Chat" RfEA6xsgL1vEu9oDqRe88/systemPrompt
            - output->"Chat" tl7qCr9Y92oWPkXWIZXqr/systemPrompt
            - output->"Get Question List" AGmIpXCDl_sHyhLIjtXie/systemPrompt
          visualData: -1600.0835425653868/-718.8751748082445/300/554
        '[b3NT35IXmv6oqd2kSitLM]:coalesce "Coalesce"':
          outgoingConnections:
            - output->"Text" VszyH3-ShGM2DBZw1v7VW/qa2
          visualData: 1425/423/150/595
        '[eJ4oKMy5hG51V_63xGOf_]:chat "Chat"':
          data:
            cache: true
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0
            stop: ""
            temperature: 0
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          outgoingConnections:
            - response->"Text" DYYQPYCdQyMP2w5Bh2QOO/steps
          visualData: 3800.927073268016/366.97261735496113/200/673
        '[eeqzpCkSxMDTin0sOghhU]:prompt "Prompt Question List"':
          data:
            promptText: |-
              {{context}}

              I have the following question or request:

              """
              {{request}}
              """

              You asked these questions:

              {{questions}}

              Here are your answers:

              {{answers}}

              Do you have any additional questions? Or followup questions?
            type: user
            useTypeInput: false
          outgoingConnections:
            - output->"Chat" tl7qCr9Y92oWPkXWIZXqr/prompt
          visualData: -518.606438840874/61.927116736475924/485.03013745499055/556
        '[erjoEqmUs-QcW3xxYqdKW]:extractObjectPath "Extract Object Path"':
          data:
            path: $.yamlDocument[*].answer
            usePathInput: false
          outgoingConnections:
            - all_matches->"Text" Y5dFBHDnIc0yTEcNA87W3/input
          visualData: 722.5034572418909/-34.12901726693207/215.11762658923863/613
        '[gMe54j9zmTUGFU1t_1viB]:text "Text"':
          data:
            text: >-
              You have Chat API functions available to you to use. You may call
              one of these functions to get a response from the system:


              - command: READ_FILES
                arguments:
                  files:
                    - file_name.ts
                description: Reads a file and returns with its contents. Files must be an array with one file per line.
              - command: TAKE_NOTE_FOR_SELF
                arguments:
                  notes: Here is some note to take for youself.
              - command: THINK_OUT_LOUD
                arguments:
                  notes: Here is some text to remember for later.
              - command: WRITE_FILE
                arguments:
                  file: file_name.ts
                contents: |
                  contents of the file
                description: Writes text to the specified file. You must write the entire file contents at once. You must not comment out sections of the file - everything needs to be implemented.
              - command: ASK_FOR_FEEDBACK
                arguments:
                  message: The message for the user
                description: Asks the user questions or for feedback on what you are doing. This can help make sure your plan is good.
          outgoingConnections:
            - output->"Prompt" YFPEXiTSWNawBUvgAF0_F/functions
            - output->"Text" 7vNigNnYPyllzuKCncBQd/functions
          visualData: 1027.1890763146014/825.3197409716632/529/675
        '[jj8mjjtKJPlcHzr64IYWp]:text "Text"':
          data:
            text: >
              Here is some text:


              """

              {{text}}

              """


              If there are any questions in the document, convert this into a YAML document with the following structure:


              ```yaml

              yamlDocument:
                has-questions: true
                questions:
                  - This is one questions
                  - This is another question
              ```


              If there are no questions, convert it to this structure:


              ```yaml

              yamlDocument:
                has-questions: false
              ```
          outgoingConnections:
            - output->"Chat" noeONFrQRJhnNOIqzfe0v/prompt
          visualData: 321.75173175600827/110.2229471496421/300/565
        '[klbBsHw6M8_i7Jz1aYovd]:extractYaml "Extract YAML"':
          data:
            rootPropertyName: yamlDocument
          outgoingConnections:
            - output->"Extract Object Path" Hqpwl6OBs8GdL917iMJNw/object
            - output->"Extract Object Path" erjoEqmUs-QcW3xxYqdKW/object
          visualData: 529.2154812835439/-228.29406204867558/147.72528347633283/609
        '[koUO8WjYqJBw5m1DqUzgv]:text "Text"':
          data:
            text: You are an advanced AI for creating and updating task lists to accomplish
              programming tasks. You interact with the chat API to accomplish
              your tasks. You use the chat API to take notes for yourself and to
              think out loud about what you are doing. You are very detailed in
              your explanations and steps.
          outgoingConnections:
            - output->"Chat" B7bKOsVis46f2wZGjHok0/systemPrompt
            - output->"Chat" wnITtjycZ2FJpLYSThTVo/systemPrompt
          visualData: 2274.1767851688805/-464.34829550512757/300/618
        '[kshUlDyeosoaczIy5rp_C]:match "Match"':
          data:
            caseCount: 2
            cases:
              - "true"
              - "false"
          outgoingConnections:
            - case1->"If" CQ11jAUax-dtEvf7Jw3Ux/if
            - case2->"If" 6UTrt-tWAv0UmymCmjyGg/if
          visualData: 815.6277136675187/481.2361797361615/177.18989855369773/581
        '[mpzTKKufP2XKwIToZ6C4x]:prompt "Prompt"':
          data:
            promptText: "{{input}}"
            type: assistant
            useTypeInput: false
          outgoingConnections:
            - output->"Assemble Prompt" H7i41CNQTM5fM5KcJDwfq/message2
          visualData: 2936.650166754064/58.192032198931265/132/639
        '[nmuXZqWZghE8cxrYC1BRW]:text "Text"':
          data:
            text: None
          outgoingConnections:
            - output->"If" 6UTrt-tWAv0UmymCmjyGg/value
          visualData: 932/700/138/590
        '[noeONFrQRJhnNOIqzfe0v]:chat "Chat"':
          data:
            cache: true
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0
            stop: ""
            temperature: 0
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          outgoingConnections:
            - response->"Extract YAML" U0bz-jzJ615Vgm78Lpley/input
          visualData: 693.2893628468472/281.2261400604991/200/566
        '[oFk-Eg5dbpoWQCK9hx_gs]:text "Text"':
          data:
            text: |-
              Here is the list of steps I will be doing:

              {{steps}}

              Do you have any changes I should make?
          outgoingConnections:
            - output->"User Input" Z115WdCClukxy33gFZQLm/questions
          visualData: 3704.1400052653166/5.251166476408985/237.2099079807631/663
        '[pqxbw_BaWOjpkd3G8WnRT]:text "Text"':
          data:
            text: "{{input}}"
          outgoingConnections:
            - output->"Text" 4-0zbzUfiK5tL1R22gT2z/answers
            - output->"Text" ZGfDzvqbUcnQunNNplfBA/answers
          visualData: 1141.592660493523/-32.3165797585659/109.04166402015858/679
        '[sMP1SZoQAgvMH3UtERway]:text "Text"':
          data:
            text: |-
              Here is a list of steps:

              """
              {{steps}}
              """

              Here are required changes to make to the steps:

              """
              {{feedback}}
              """

              Please write the steps, with the required changes made.
          outgoingConnections:
            - output->"Chat" eJ4oKMy5hG51V_63xGOf_/prompt
          visualData: 3449.6025964744235/222.3511968045764/300/669
        '[tl7qCr9Y92oWPkXWIZXqr]:chat "Chat"':
          data:
            cache: true
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0
            stop: ""
            temperature: 0
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          outgoingConnections:
            - response->"Text" jj8mjjtKJPlcHzr64IYWp/text
          visualData: 58.74579830891864/277.5903593432446/200/558
        '[wTZ8HUHOjU5COaWxaohVb]:chat "Chat"':
          data:
            cache: true
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0
            stop: ""
            temperature: 0
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          outgoingConnections:
            - response->"Extract YAML" klbBsHw6M8_i7Jz1aYovd/input
          visualData: 852.9285172548934/-523.9745290372787/200/608
        '[wVMd8FhPjQOvVlnHxZR_L]:text "Text"':
          data:
            text: You are an advanced AI that combines two pieces of text. You do not leave
              any details out.
          outgoingConnections:
            - output->"Chat" eJ4oKMy5hG51V_63xGOf_/systemPrompt
          visualData: 3089.9485001028697/288.8805450446473/304.0744894139352/672
        '[wnITtjycZ2FJpLYSThTVo]:chat "Chat"':
          data:
            cache: true
            frequencyPenalty: 0.2
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0.2
            stop: ""
            temperature: 0.2
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          outgoingConnections:
            - response->"Prompt" mpzTKKufP2XKwIToZ6C4x/input
            - response->"Text" VrlVJ3R4e5XeyDKqHmrjz/steps
          visualData: 2665.869382184409/17.864793208914165/200/635
        '[xxSFUqUpaoGhRrU__HAUO]:chat "Chat"':
          data:
            cache: true
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-4
            presencePenalty: 0
            stop: ""
            temperature: 0
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          outgoingConnections:
            - response->"Prompt" YFPEXiTSWNawBUvgAF0_F/qanda
          visualData: 1571.1932071403805/-284.56226186771005/200/628
        '[yWAKuzopcX8M17WGEDi-6]:subGraph "Execute Task List"':
          data:
            graphId: yoe9VPnjUULQacHSolgiL
          visualData: 5580.14785799502/-48.341959799326155/300/655
        '[zE8Wp38T4EAR0doXvx4zT]:text "Text"':
          data:
            text: You are an advanced AI YAML generator tool that takes in inputs and
              produces valid YAML based on a question. You leave nothing out.
              Your conversion is verbatim.
          outgoingConnections:
            - output->"Chat" At8pw5kk0skpMBKEBPgZp/systemPrompt
          visualData: 4488.321151175641/-135.07604537061488/300/659
        '[zc7_qtAZEqumCFsUYVqt7]:prompt "Base Context"':
          data:
            promptText: >-
              I am working on developing an AI storyboarding tool that allows
              users to create a series of prompts for a language model in a
              choose-your-own-adventure format. The tool is inspired by
              node-based editors, like the one found in Blender, where users can
              create nodes on a page that have inputs and outputs that can be
              connected by wires to form a web of connections between the
              prompts and the AI. Each node can be edited, and when editing, a
              larger window pops up with a text editor where users can tweak
              various aspects of the prompt that will be fed to the AI. This
              tool will provide a user-friendly interface for crafting
              interactive stories with an AI language model. Here is a tree of
              my current files for context. If you would like the contents of
              any of these files, please ask. The app is dark-themed and the
              colors are available in index.css. I'm using Emotion for CSS.


              Every node is split into two files. The file in `core`, such as `core/src/model/nodes/TextNode.ts`, contains the main implementation of the node's functionality. The file in `app`, such as `app/src/components/nodes/TextNode.tsx`, contains three react UI components related to showing the node in the application.
            type: user
            useTypeInput: false
          outgoingConnections:
            - output->"Get Question List" AGmIpXCDl_sHyhLIjtXie/message1
            - output->"Prompt Question List" 5-z53LCARmASeWft67PEH/context
            - output->"Prompt Question List" KbotaIf5SLrUkOubRve1x/context
            - output->"Prompt Question List" eeqzpCkSxMDTin0sOghhU/context
            - output->"Prompt" YFPEXiTSWNawBUvgAF0_F/context
          visualData: -2228.0616865013358/-551.6344392043734/528/535
    zdzAYsxAtpaIcWCfgFZpz:
      metadata:
        description: ""
        id: zdzAYsxAtpaIcWCfgFZpz
        name: Test Digest
      nodes:
        '[LCcfd6Kcj2Rw0q1glhv6O]:subGraph "Digest File"':
          data:
            graphId: HXjZhpWO0hluMiDY6pneE
          visualData: 589/377/300/3
        '[tNLFIIZaLhPiLXTzaSX4V]:userInput "User Input"':
          data:
            prompt: File
            useInput: false
          outgoingConnections:
            - output->"Digest File" LCcfd6Kcj2Rw0q1glhv6O/fileMatch
          visualData: 284/377/250/2
  metadata:
    description: Project for Rivet itself
    id: KufOYoZj8bXSme0gx4W-e
    title: Rivet
